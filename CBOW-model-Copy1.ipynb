{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9640057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install a library to work with docx files\n",
    "# pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e18481f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohammad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "from utils import softmax, relu, get_batches, compute_pca, get_dict, cosine_similarity, euclidean_distance\n",
    "import re #  Load the Regex-modul\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3efb58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23310448",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/almizan_processed.txt'\n",
    "def get_text(path):\n",
    "    data = []\n",
    "    with open('./data/almizan_processed.txt', 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2142ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and process the data\n",
    "def tokenize(corpus):\n",
    "    data = \"\\n\".join(corpus)\n",
    "    data = nltk.word_tokenize(data) \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e42338a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4147452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 155735 \n",
      " ['بسم', 'الله', 'الرحمن', 'الرحيم', 'پايگاه', 'قرآن', 'شناسي', 'حوزه', 'علميه', 'ميبد', 'تفسير', 'الميزان', 'السيد', 'الطباطبائي', 'الجز', 'الثاني', 'سورة', 'البقرة', 'يأيها', 'الذين', 'امنوا', 'كتب', 'عليكم', 'الصيام', 'كما', 'كتب', 'على', 'الذين', 'من', 'قبلكم']\n"
     ]
    }
   ],
   "source": [
    "data = get_text(path)\n",
    "data = tokenize(data)\n",
    "# data = stemming(data)\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:30]) #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6c23d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  20449\n",
      "Most frequent tokens:  [('و', 16135), ('في', 4297), ('من', 4240), ('الله', 2444), ('أن', 2262), ('لا', 2157), ('على', 2145), ('.', 2046), ('ما', 1815), ('تعالى', 1806), ('عن', 1445), ('إلى', 1381), ('قوله', 1292), ('هو', 1224), ('عليه', 1089), ('قال', 1060), ('ذلك', 850), ('كان', 831), ('أو', 736), ('إن', 720)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \", len(fdist))\n",
    "print(\"Most frequent tokens: \", fdist.most_common(20)) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0922823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  20449\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43c975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'way' :  12083\n",
      "Word which has index 12578:  صراط\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "# replace هذا with 'this' because of nice format otherwise we have an ugly format\n",
    "print(f\"Index of the word 'way' :  {word2Ind['صراط']}\")\n",
    "print(f\"Word which has index 12578:  {Ind2word[12083]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7c085",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b00fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=9):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    # W1_1 has shape (N,V)\n",
    "    W1_1 = np.random.rand(N,V)\n",
    "    \n",
    "    # W1_2 has shape (N,V)\n",
    "    W1_2 = np.random.rand(N,V)\n",
    "    \n",
    "    # W1_3 has shape (N,V)\n",
    "    W1_3 = np.random.rand(N,V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1_1, W1_2, W1_3, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fff23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test initialize_model function.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd8f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1_1, W1_2, W1_3, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    # Calculate h\n",
    "    h1 = np.dot(W1_1, x) + b1\n",
    "    h2 = np.dot(W1_2, x) + b1\n",
    "    h3 = np.dot(W1_3, x) + b1\n",
    "    \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h1 = relu(h1)\n",
    "    h2 = relu(h2)\n",
    "    h3 = relu(h3)\n",
    "    \n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h1) + np.dot(W2, h2)+ np.dot(W2, h3) + b2\n",
    "\n",
    "    return z, h1, h2, h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d1fed16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "call forward_prop\n",
      "\n",
      "z has shape (3, 1)\n",
      "z has values:\n",
      "[[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]]\n",
      "\n",
      "h has shape (2, 1)\n",
      "h has values:\n",
      "[[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward_prop function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# call function\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"h has shape {tmp_h.shape}\")\n",
    "print(\"h has values:\")\n",
    "print(tmp_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f3aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = -1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "91f925d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_x.shape (20449, 4)\n",
      "tmp_y.shape (20449, 4)\n",
      "tmp_W1.shape (50, 20449)\n",
      "tmp_W2.shape (20449, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (20449, 1)\n",
      "tmp_z.shape: (20449, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "tmp_yhat.shape: (20449, 4)\n",
      "call compute_cost\n",
      "tmp_cost 13.4959\n"
     ]
    }
   ],
   "source": [
    "# Test the compute_cost function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "print(\"call compute_cost\")\n",
    "print(f\"tmp_cost {tmp_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1bebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h1, h2, h3, W1_1, W1_2, W1_3, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # Apply relu to l1\n",
    "    l1 = relu(l1)\n",
    "\n",
    "    # compute the gradient for W1_1\n",
    "    grad_W1_1 = (1/batch_size) * np.dot(l1, x.T)\n",
    "    \n",
    "    # compute the gradient for W1_2\n",
    "    grad_W1_2 = (1/batch_size) * np.dot(l1, x.T)\n",
    "    \n",
    "    # compute the gradient for W1_3\n",
    "    grad_W1_3 = (1/batch_size) * np.dot(l1, x.T)\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = (1/batch_size) * np.dot(yhat - y, h1.T + h2.T + h2.T)\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = (1/batch_size) * np.sum(l1, axis=1, keepdims=True)\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = (1/batch_size) * np.sum((yhat - y), axis=1, keepdims=True)\n",
    "    \n",
    "    return grad_W1_1, grad_W1_2, grad_W1_3, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7299b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "tmp_x.shape (20449, 4)\n",
      "tmp_y.shape (20449, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 20449)\n",
      "tmp_W2.shape (20449, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (20449, 1)\n",
      "\n",
      "Forwad prop to get z and h\n",
      "tmp_z.shape: (20449, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "\n",
      "Get yhat by calling softmax\n",
      "tmp_yhat.shape: (20449, 4)\n",
      "\n",
      "call back_prop\n",
      "tmp_grad_W1.shape (50, 20449)\n",
      "tmp_grad_W2.shape (20449, 50)\n",
      "tmp_grad_b1.shape (50, 1)\n",
      "tmp_grad_b2.shape (20449, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the back_prop function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "\n",
    "# get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "print(\"get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Forwad prop to get z and h\")\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Get yhat by calling softmax\")\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_m = (2*tmp_C)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "print()\n",
    "print(\"call back_prop\")\n",
    "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9c880bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
    "                     random_seed=9, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: implementation of the function to perform forward propagation\n",
    "        softmax: implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1_1, W1_2, W1_3, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "#     batch_size = 512\n",
    "    batch_size = 2048\n",
    "    iters = 0\n",
    "    C = 10\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h1, h2, h3 = forward_prop(x, W1_1, W1_2, W1_3, W2, b1, b2)\n",
    "                \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 5 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f} --- alpha: {alpha}\")\n",
    "            \n",
    "        # get gradients\n",
    "        grad_W1_1, grad_W1_2, grad_W1_3, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h1, h2, h3, W1_1, W1_2, W1_3, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1_1 = W1_1 - alpha * grad_W1_1\n",
    "        W1_2 = W1_2 - alpha * grad_W1_2\n",
    "        W1_3 = W1_3 - alpha * grad_W1_3\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 50 == 0:\n",
    "            alpha *= 0.75**np.floor(iters/50)\n",
    "          \n",
    "            \n",
    "    return W1_1, W1_2, W1_3, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53784407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 101.375138 --- alpha: 0.03\n",
      "iters: 20 cost: 69.733171 --- alpha: 0.03\n",
      "iters: 30 cost: 62.011337 --- alpha: 0.03\n",
      "iters: 40 cost: 40.096269 --- alpha: 0.03\n",
      "iters: 50 cost: 74.260180 --- alpha: 0.03\n",
      "iters: 60 cost: 48.492908 --- alpha: 0.03\n",
      "iters: 70 cost: 30.582832 --- alpha: 0.03\n",
      "iters: 80 cost: 17.727734 --- alpha: 0.03\n",
      "iters: 90 cost: 24.935919 --- alpha: 0.03\n",
      "iters: 100 cost: 26.309843 --- alpha: 0.03\n",
      "iters: 110 cost: 22.834521 --- alpha: 0.0198\n",
      "iters: 120 cost: 5.648274 --- alpha: 0.0198\n",
      "iters: 130 cost: 15.566277 --- alpha: 0.0198\n",
      "iters: 140 cost: 19.370512 --- alpha: 0.0198\n",
      "iters: 150 cost: 24.542829 --- alpha: 0.0198\n",
      "iters: 160 cost: 12.445880 --- alpha: 0.0198\n",
      "iters: 170 cost: 3.225476 --- alpha: 0.0198\n",
      "iters: 180 cost: 13.001639 --- alpha: 0.0198\n",
      "iters: 190 cost: 8.964290 --- alpha: 0.0198\n",
      "iters: 200 cost: 14.237978 --- alpha: 0.0198\n",
      "iters: 210 cost: 13.607223 --- alpha: 0.013068000000000001\n",
      "iters: 220 cost: 8.481114 --- alpha: 0.013068000000000001\n",
      "iters: 230 cost: 6.489887 --- alpha: 0.013068000000000001\n",
      "iters: 240 cost: 12.945826 --- alpha: 0.013068000000000001\n",
      "iters: 250 cost: 15.060095 --- alpha: 0.013068000000000001\n",
      "iters: 260 cost: 12.170047 --- alpha: 0.013068000000000001\n",
      "iters: 270 cost: 6.931434 --- alpha: 0.013068000000000001\n",
      "iters: 280 cost: 11.513503 --- alpha: 0.013068000000000001\n",
      "iters: 290 cost: 9.076189 --- alpha: 0.013068000000000001\n",
      "iters: 300 cost: 13.845225 --- alpha: 0.013068000000000001\n",
      "iters: 310 cost: 11.964073 --- alpha: 0.008624880000000001\n",
      "iters: 320 cost: 14.665612 --- alpha: 0.008624880000000001\n",
      "iters: 330 cost: 12.114363 --- alpha: 0.008624880000000001\n",
      "iters: 340 cost: 6.096043 --- alpha: 0.008624880000000001\n",
      "iters: 350 cost: 7.268057 --- alpha: 0.008624880000000001\n",
      "iters: 360 cost: 3.925799 --- alpha: 0.008624880000000001\n",
      "iters: 370 cost: 8.915001 --- alpha: 0.008624880000000001\n",
      "iters: 380 cost: 8.732717 --- alpha: 0.008624880000000001\n",
      "iters: 390 cost: 9.556047 --- alpha: 0.008624880000000001\n",
      "iters: 400 cost: 7.024840 --- alpha: 0.008624880000000001\n",
      "iters: 410 cost: 11.407262 --- alpha: 0.005692420800000001\n",
      "iters: 420 cost: 10.021361 --- alpha: 0.005692420800000001\n",
      "iters: 430 cost: 8.303498 --- alpha: 0.005692420800000001\n",
      "iters: 440 cost: 10.892231 --- alpha: 0.005692420800000001\n",
      "iters: 450 cost: 8.986904 --- alpha: 0.005692420800000001\n",
      "iters: 460 cost: 9.701423 --- alpha: 0.005692420800000001\n",
      "iters: 470 cost: 8.032367 --- alpha: 0.005692420800000001\n",
      "iters: 480 cost: 5.320231 --- alpha: 0.005692420800000001\n",
      "iters: 490 cost: 9.898406 --- alpha: 0.005692420800000001\n",
      "iters: 500 cost: 10.313895 --- alpha: 0.005692420800000001\n",
      "iters: 510 cost: 10.933477 --- alpha: 0.003756997728000001\n",
      "iters: 520 cost: 8.252496 --- alpha: 0.003756997728000001\n",
      "iters: 530 cost: 5.926586 --- alpha: 0.003756997728000001\n",
      "iters: 540 cost: 9.317940 --- alpha: 0.003756997728000001\n",
      "iters: 550 cost: 10.805562 --- alpha: 0.003756997728000001\n",
      "iters: 560 cost: 10.683961 --- alpha: 0.003756997728000001\n",
      "iters: 570 cost: 6.610093 --- alpha: 0.003756997728000001\n",
      "iters: 580 cost: 10.061499 --- alpha: 0.003756997728000001\n",
      "iters: 590 cost: 10.687552 --- alpha: 0.003756997728000001\n",
      "iters: 600 cost: 10.863481 --- alpha: 0.003756997728000001\n",
      "iters: 610 cost: 9.835162 --- alpha: 0.002479618500480001\n",
      "iters: 620 cost: 5.528561 --- alpha: 0.002479618500480001\n",
      "iters: 630 cost: 9.855409 --- alpha: 0.002479618500480001\n",
      "iters: 640 cost: 9.445042 --- alpha: 0.002479618500480001\n",
      "iters: 650 cost: 10.717760 --- alpha: 0.002479618500480001\n",
      "iters: 660 cost: 8.075517 --- alpha: 0.002479618500480001\n",
      "iters: 670 cost: 8.119358 --- alpha: 0.002479618500480001\n",
      "iters: 680 cost: 10.689624 --- alpha: 0.002479618500480001\n",
      "iters: 690 cost: 9.043930 --- alpha: 0.002479618500480001\n",
      "iters: 700 cost: 9.177533 --- alpha: 0.002479618500480001\n",
      "iters: 710 cost: 9.522107 --- alpha: 0.0016365482103168007\n",
      "iters: 720 cost: 8.262158 --- alpha: 0.0016365482103168007\n",
      "iters: 730 cost: 9.153364 --- alpha: 0.0016365482103168007\n",
      "iters: 740 cost: 9.435101 --- alpha: 0.0016365482103168007\n",
      "iters: 750 cost: 10.070637 --- alpha: 0.0016365482103168007\n",
      "iters: 760 cost: 10.131457 --- alpha: 0.0016365482103168007\n",
      "iters: 770 cost: 10.186644 --- alpha: 0.0016365482103168007\n",
      "iters: 780 cost: 9.285686 --- alpha: 0.0016365482103168007\n",
      "iters: 790 cost: 9.511309 --- alpha: 0.0016365482103168007\n",
      "iters: 800 cost: 10.885037 --- alpha: 0.0016365482103168007\n",
      "iters: 810 cost: 10.060268 --- alpha: 0.0010801218188090885\n",
      "iters: 820 cost: 8.857588 --- alpha: 0.0010801218188090885\n",
      "iters: 830 cost: 9.223527 --- alpha: 0.0010801218188090885\n",
      "iters: 840 cost: 8.554009 --- alpha: 0.0010801218188090885\n",
      "iters: 850 cost: 10.459356 --- alpha: 0.0010801218188090885\n",
      "iters: 860 cost: 10.099847 --- alpha: 0.0010801218188090885\n",
      "iters: 870 cost: 8.345961 --- alpha: 0.0010801218188090885\n",
      "iters: 880 cost: 8.584284 --- alpha: 0.0010801218188090885\n",
      "iters: 890 cost: 9.551049 --- alpha: 0.0010801218188090885\n",
      "iters: 900 cost: 9.188536 --- alpha: 0.0010801218188090885\n",
      "iters: 910 cost: 10.451288 --- alpha: 0.0007128804004139984\n",
      "iters: 920 cost: 9.431841 --- alpha: 0.0007128804004139984\n",
      "iters: 930 cost: 9.284151 --- alpha: 0.0007128804004139984\n",
      "iters: 940 cost: 10.372012 --- alpha: 0.0007128804004139984\n",
      "iters: 950 cost: 9.988530 --- alpha: 0.0007128804004139984\n",
      "iters: 960 cost: 9.123754 --- alpha: 0.0007128804004139984\n",
      "iters: 970 cost: 9.420505 --- alpha: 0.0007128804004139984\n",
      "iters: 980 cost: 10.225787 --- alpha: 0.0007128804004139984\n",
      "iters: 990 cost: 8.705529 --- alpha: 0.0007128804004139984\n",
      "iters: 1000 cost: 9.124194 --- alpha: 0.0007128804004139984\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "data = get_text(path)\n",
    "data = tokenize(data)\n",
    "# C = 4\n",
    "# batch_size = 1024\n",
    "# alpha = 0.03\n",
    "N = 300\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1_1, W1_2, W1_3, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4e6a3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 66.918643 --- alpha: 0.001\n",
      "iters: 20 cost: 43.267170 --- alpha: 0.001\n",
      "iters: 30 cost: 29.231779 --- alpha: 0.001\n",
      "iters: 40 cost: 51.534444 --- alpha: 0.001\n",
      "iters: 50 cost: 66.261764 --- alpha: 0.001\n",
      "iters: 60 cost: 40.553645 --- alpha: 0.001\n",
      "iters: 70 cost: 35.696168 --- alpha: 0.001\n",
      "iters: 80 cost: 27.725833 --- alpha: 0.001\n",
      "iters: 90 cost: 50.026515 --- alpha: 0.001\n",
      "iters: 100 cost: 47.167521 --- alpha: 0.001\n",
      "iters: 110 cost: 30.696450 --- alpha: 0.00066\n",
      "iters: 120 cost: 39.898838 --- alpha: 0.00066\n",
      "iters: 130 cost: 62.846918 --- alpha: 0.00066\n",
      "iters: 140 cost: 25.971666 --- alpha: 0.00066\n",
      "iters: 150 cost: 29.949782 --- alpha: 0.00066\n",
      "iters: 160 cost: 22.250742 --- alpha: 0.00066\n",
      "iters: 170 cost: 35.600634 --- alpha: 0.00066\n",
      "iters: 180 cost: 33.964402 --- alpha: 0.00066\n",
      "iters: 190 cost: 71.404675 --- alpha: 0.00066\n",
      "iters: 200 cost: 33.275135 --- alpha: 0.00066\n",
      "iters: 210 cost: 71.874649 --- alpha: 0.0004356\n",
      "iters: 220 cost: 30.940600 --- alpha: 0.0004356\n",
      "iters: 230 cost: 24.117955 --- alpha: 0.0004356\n",
      "iters: 240 cost: 55.917092 --- alpha: 0.0004356\n",
      "iters: 250 cost: 32.798009 --- alpha: 0.0004356\n",
      "iters: 260 cost: 51.821966 --- alpha: 0.0004356\n",
      "iters: 270 cost: 67.356863 --- alpha: 0.0004356\n",
      "iters: 280 cost: 45.716205 --- alpha: 0.0004356\n",
      "iters: 290 cost: 51.955007 --- alpha: 0.0004356\n",
      "iters: 300 cost: 21.589468 --- alpha: 0.0004356\n",
      "iters: 310 cost: 59.827234 --- alpha: 0.00028749600000000004\n",
      "iters: 320 cost: 19.786244 --- alpha: 0.00028749600000000004\n",
      "iters: 330 cost: 29.575873 --- alpha: 0.00028749600000000004\n",
      "iters: 340 cost: 49.211355 --- alpha: 0.00028749600000000004\n",
      "iters: 350 cost: 58.798000 --- alpha: 0.00028749600000000004\n",
      "iters: 360 cost: 49.394441 --- alpha: 0.00028749600000000004\n",
      "iters: 370 cost: 55.512586 --- alpha: 0.00028749600000000004\n",
      "iters: 380 cost: 31.265497 --- alpha: 0.00028749600000000004\n",
      "iters: 390 cost: 39.448222 --- alpha: 0.00028749600000000004\n",
      "iters: 400 cost: 32.518865 --- alpha: 0.00028749600000000004\n",
      "iters: 410 cost: 17.276043 --- alpha: 0.00018974736000000004\n",
      "iters: 420 cost: 2.156373 --- alpha: 0.00018974736000000004\n",
      "iters: 430 cost: 32.057323 --- alpha: 0.00018974736000000004\n",
      "iters: 440 cost: 63.837940 --- alpha: 0.00018974736000000004\n",
      "iters: 450 cost: 55.215058 --- alpha: 0.00018974736000000004\n",
      "iters: 460 cost: 20.488116 --- alpha: 0.00018974736000000004\n",
      "iters: 470 cost: 34.355879 --- alpha: 0.00018974736000000004\n",
      "iters: 480 cost: 19.060074 --- alpha: 0.00018974736000000004\n",
      "iters: 490 cost: 33.640724 --- alpha: 0.00018974736000000004\n",
      "iters: 500 cost: 18.648212 --- alpha: 0.00018974736000000004\n",
      "iters: 510 cost: 34.379023 --- alpha: 0.00012523325760000002\n",
      "iters: 520 cost: 37.294987 --- alpha: 0.00012523325760000002\n",
      "iters: 530 cost: 14.238058 --- alpha: 0.00012523325760000002\n",
      "iters: 540 cost: 3.689641 --- alpha: 0.00012523325760000002\n",
      "iters: 550 cost: 44.704325 --- alpha: 0.00012523325760000002\n",
      "iters: 560 cost: 24.152813 --- alpha: 0.00012523325760000002\n",
      "iters: 570 cost: 45.022479 --- alpha: 0.00012523325760000002\n",
      "iters: 580 cost: 2.241840 --- alpha: 0.00012523325760000002\n",
      "iters: 590 cost: 51.588556 --- alpha: 0.00012523325760000002\n",
      "iters: 600 cost: 4.202445 --- alpha: 0.00012523325760000002\n",
      "iters: 610 cost: 34.130770 --- alpha: 8.265395001600001e-05\n",
      "iters: 620 cost: 42.913773 --- alpha: 8.265395001600001e-05\n",
      "iters: 630 cost: 30.553539 --- alpha: 8.265395001600001e-05\n",
      "iters: 640 cost: 38.783952 --- alpha: 8.265395001600001e-05\n",
      "iters: 650 cost: 36.179972 --- alpha: 8.265395001600001e-05\n",
      "iters: 660 cost: 44.191380 --- alpha: 8.265395001600001e-05\n",
      "iters: 670 cost: 45.098551 --- alpha: 8.265395001600001e-05\n",
      "iters: 680 cost: 38.407923 --- alpha: 8.265395001600001e-05\n",
      "iters: 690 cost: 7.107242 --- alpha: 8.265395001600001e-05\n",
      "iters: 700 cost: 13.088864 --- alpha: 8.265395001600001e-05\n",
      "iters: 710 cost: 11.702997 --- alpha: 5.4551607010560014e-05\n",
      "iters: 720 cost: 1.992084 --- alpha: 5.4551607010560014e-05\n",
      "iters: 730 cost: 55.979054 --- alpha: 5.4551607010560014e-05\n",
      "iters: 740 cost: 2.444558 --- alpha: 5.4551607010560014e-05\n",
      "iters: 750 cost: 46.413335 --- alpha: 5.4551607010560014e-05\n",
      "iters: 760 cost: 34.051894 --- alpha: 5.4551607010560014e-05\n",
      "iters: 770 cost: 2.160083 --- alpha: 5.4551607010560014e-05\n",
      "iters: 780 cost: 35.979893 --- alpha: 5.4551607010560014e-05\n",
      "iters: 790 cost: 26.189317 --- alpha: 5.4551607010560014e-05\n",
      "iters: 800 cost: 41.337075 --- alpha: 5.4551607010560014e-05\n",
      "iters: 810 cost: 2.348623 --- alpha: 3.6004060626969614e-05\n",
      "iters: 820 cost: 39.594592 --- alpha: 3.6004060626969614e-05\n",
      "iters: 830 cost: 39.535487 --- alpha: 3.6004060626969614e-05\n",
      "iters: 840 cost: 2.326628 --- alpha: 3.6004060626969614e-05\n",
      "iters: 850 cost: 36.604803 --- alpha: 3.6004060626969614e-05\n",
      "iters: 860 cost: 4.922287 --- alpha: 3.6004060626969614e-05\n",
      "iters: 870 cost: 34.062124 --- alpha: 3.6004060626969614e-05\n",
      "iters: 880 cost: 26.230217 --- alpha: 3.6004060626969614e-05\n",
      "iters: 890 cost: 16.866833 --- alpha: 3.6004060626969614e-05\n",
      "iters: 900 cost: 33.321616 --- alpha: 3.6004060626969614e-05\n",
      "iters: 910 cost: 44.399469 --- alpha: 2.3762680013799946e-05\n",
      "iters: 920 cost: 66.892929 --- alpha: 2.3762680013799946e-05\n",
      "iters: 930 cost: 63.913926 --- alpha: 2.3762680013799946e-05\n",
      "iters: 940 cost: 2.800326 --- alpha: 2.3762680013799946e-05\n",
      "iters: 950 cost: 55.157693 --- alpha: 2.3762680013799946e-05\n",
      "iters: 960 cost: 3.024322 --- alpha: 2.3762680013799946e-05\n",
      "iters: 970 cost: 33.593837 --- alpha: 2.3762680013799946e-05\n",
      "iters: 980 cost: 18.834311 --- alpha: 2.3762680013799946e-05\n",
      "iters: 990 cost: 17.796812 --- alpha: 2.3762680013799946e-05\n",
      "iters: 1000 cost: 5.053205 --- alpha: 2.3762680013799946e-05\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "data = get_text(path)\n",
    "data = tokenize(data)\n",
    "# C = 10\n",
    "# batch_size = 512\n",
    "# alpha = 0.001\n",
    "N = 300\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1_1, W1_2, W1_3, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b80c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 5 cost: 41.706129 --- alpha: 0.001\n",
      "iters: 10 cost: 66.918643 --- alpha: 0.001\n",
      "iters: 15 cost: 70.892572 --- alpha: 0.00099\n",
      "iters: 20 cost: 43.278800 --- alpha: 0.00099\n",
      "iters: 25 cost: 15.232660 --- alpha: 0.0009801\n",
      "iters: 30 cost: 29.287249 --- alpha: 0.0009801\n",
      "iters: 35 cost: 37.237419 --- alpha: 0.000970299\n",
      "iters: 40 cost: 51.578663 --- alpha: 0.000970299\n",
      "iters: 45 cost: 18.045089 --- alpha: 0.0009605960099999999\n",
      "iters: 50 cost: 66.325030 --- alpha: 0.0009605960099999999\n",
      "iters: 55 cost: 33.833182 --- alpha: 0.0009509900498999999\n",
      "iters: 60 cost: 40.635341 --- alpha: 0.0009509900498999999\n",
      "iters: 65 cost: 51.619576 --- alpha: 0.0009414801494009999\n",
      "iters: 70 cost: 35.799551 --- alpha: 0.0009414801494009999\n",
      "iters: 75 cost: 69.136048 --- alpha: 0.0009320653479069899\n",
      "iters: 80 cost: 27.894763 --- alpha: 0.0009320653479069899\n",
      "iters: 85 cost: 40.811603 --- alpha: 0.00092274469442792\n",
      "iters: 90 cost: 50.164309 --- alpha: 0.00092274469442792\n",
      "iters: 95 cost: 39.208214 --- alpha: 0.0009135172474836408\n",
      "iters: 100 cost: 47.324990 --- alpha: 0.0009135172474836408\n",
      "iters: 105 cost: 35.679636 --- alpha: 0.0009043820750088043\n",
      "iters: 110 cost: 30.756942 --- alpha: 0.0009043820750088043\n",
      "iters: 115 cost: 43.196299 --- alpha: 0.0008953382542587163\n",
      "iters: 120 cost: 39.879132 --- alpha: 0.0008953382542587163\n",
      "iters: 125 cost: 58.331320 --- alpha: 0.0008863848717161291\n",
      "iters: 130 cost: 62.747063 --- alpha: 0.0008863848717161291\n",
      "iters: 135 cost: 33.039679 --- alpha: 0.0008775210229989678\n",
      "iters: 140 cost: 24.574058 --- alpha: 0.0008775210229989678\n",
      "iters: 145 cost: 50.836301 --- alpha: 0.0008687458127689781\n",
      "iters: 150 cost: 29.733525 --- alpha: 0.0008687458127689781\n",
      "iters: 155 cost: 25.740671 --- alpha: 0.0008600583546412883\n",
      "iters: 160 cost: 22.247805 --- alpha: 0.0008600583546412883\n",
      "iters: 165 cost: 31.335612 --- alpha: 0.0008514577710948754\n",
      "iters: 170 cost: 35.355538 --- alpha: 0.0008514577710948754\n",
      "iters: 175 cost: 28.303312 --- alpha: 0.0008429431933839266\n",
      "iters: 180 cost: 33.143804 --- alpha: 0.0008429431933839266\n",
      "iters: 185 cost: 40.689542 --- alpha: 0.0008345137614500873\n",
      "iters: 190 cost: 69.327757 --- alpha: 0.0008345137614500873\n",
      "iters: 195 cost: 45.103936 --- alpha: 0.0008261686238355864\n",
      "iters: 200 cost: 32.879889 --- alpha: 0.0008261686238355864\n",
      "iters: 205 cost: 52.789733 --- alpha: 0.0008179069375972306\n",
      "iters: 210 cost: 70.150112 --- alpha: 0.0008179069375972306\n",
      "iters: 215 cost: 46.881913 --- alpha: 0.0008097278682212583\n",
      "iters: 220 cost: 30.380387 --- alpha: 0.0008097278682212583\n",
      "iters: 225 cost: 67.940382 --- alpha: 0.0008016305895390457\n",
      "iters: 230 cost: 21.842260 --- alpha: 0.0008016305895390457\n",
      "iters: 235 cost: 10.770858 --- alpha: 0.0007936142836436553\n",
      "iters: 240 cost: 54.212742 --- alpha: 0.0007936142836436553\n",
      "iters: 245 cost: 27.571450 --- alpha: 0.0007856781408072188\n",
      "iters: 250 cost: 32.024516 --- alpha: 0.0007856781408072188\n",
      "iters: 255 cost: 4.911663 --- alpha: 0.0007778213593991466\n",
      "iters: 260 cost: 50.962998 --- alpha: 0.0007778213593991466\n",
      "iters: 265 cost: 17.619378 --- alpha: 0.000770043145805155\n",
      "iters: 270 cost: 66.414681 --- alpha: 0.000770043145805155\n",
      "iters: 275 cost: 4.559011 --- alpha: 0.00022870281430413102\n",
      "iters: 280 cost: 44.079269 --- alpha: 0.00022870281430413102\n",
      "iters: 285 cost: 36.118387 --- alpha: 0.0002264157861610897\n",
      "iters: 290 cost: 51.176094 --- alpha: 2.037742075449807e-05\n",
      "iters: 295 cost: 24.000552 --- alpha: 2.017364654695309e-05\n",
      "iters: 300 cost: 21.098146 --- alpha: 2.017364654695309e-05\n",
      "iters: 305 cost: 52.924889 --- alpha: 5.9915730244450675e-06\n",
      "iters: 310 cost: 56.758938 --- alpha: 5.9915730244450675e-06\n",
      "iters: 315 cost: 25.716909 --- alpha: 1.779497188260185e-06\n",
      "iters: 320 cost: 18.624436 --- alpha: 5.338491564780555e-07\n",
      "iters: 325 cost: 44.557122 --- alpha: 1.585531994739825e-07\n",
      "iters: 330 cost: 29.252718 --- alpha: 4.756595984219475e-08\n",
      "iters: 335 cost: 9.488164 --- alpha: 1.412709007313184e-08\n",
      "iters: 340 cost: 48.923023 --- alpha: 4.238127021939552e-09\n",
      "iters: 345 cost: 61.569046 --- alpha: 4.195745751720156e-09\n",
      "iters: 350 cost: 58.503081 --- alpha: 4.195745751720156e-09\n",
      "iters: 355 cost: 52.405391 --- alpha: 3.738409464782659e-10\n",
      "iters: 360 cost: 49.081803 --- alpha: 3.738409464782659e-10\n",
      "iters: 365 cost: 41.549087 --- alpha: 3.7010253701348326e-10\n",
      "iters: 370 cost: 55.432273 --- alpha: 3.7010253701348326e-10\n",
      "iters: 375 cost: 42.626234 --- alpha: 1.0992045349300452e-10\n",
      "iters: 380 cost: 31.221749 --- alpha: 1.0992045349300452e-10\n",
      "iters: 385 cost: 42.019647 --- alpha: 3.264637468742234e-11\n",
      "iters: 390 cost: 39.264325 --- alpha: 9.793912406226701e-12\n",
      "iters: 395 cost: 45.248320 --- alpha: 9.695973282164435e-12\n",
      "iters: 400 cost: 32.497548 --- alpha: 9.695973282164435e-12\n",
      "iters: 405 cost: 12.991524 --- alpha: 9.59901354934279e-12\n",
      "iters: 410 cost: 17.347692 --- alpha: 2.879704064802837e-12\n",
      "iters: 415 cost: 23.138827 --- alpha: 8.552721072464426e-13\n",
      "iters: 420 cost: 1.384591 --- alpha: 8.552721072464426e-13\n",
      "iters: 425 cost: 44.428926 --- alpha: 7.620474475565802e-14\n",
      "iters: 430 cost: 32.218311 --- alpha: 7.620474475565802e-14\n",
      "iters: 435 cost: 66.632087 --- alpha: 2.2632809192430434e-14\n",
      "iters: 440 cost: 62.045128 --- alpha: 2.036952827318739e-15\n",
      "iters: 445 cost: 35.508121 --- alpha: 2.0165832990455516e-15\n",
      "iters: 450 cost: 55.003777 --- alpha: 6.049749897136654e-16\n",
      "iters: 455 cost: 1.884983 --- alpha: 5.989252398165287e-16\n",
      "iters: 460 cost: 20.760506 --- alpha: 1.796775719449586e-16\n",
      "iters: 465 cost: 27.158418 --- alpha: 5.3364238867652707e-17\n",
      "iters: 470 cost: 34.873873 --- alpha: 5.3364238867652707e-17\n",
      "iters: 475 cost: 63.834055 --- alpha: 1.5849178943692853e-17\n",
      "iters: 480 cost: 18.620994 --- alpha: 1.5849178943692853e-17\n",
      "iters: 485 cost: 3.006977 --- alpha: 1.5690687154255924e-17\n",
      "iters: 490 cost: 33.908925 --- alpha: 1.412161843883033e-18\n",
      "iters: 495 cost: 13.475689 --- alpha: 1.3980402254442027e-18\n",
      "iters: 500 cost: 18.874643 --- alpha: 4.194120676332608e-19\n",
      "iters: 505 cost: 3.197857 --- alpha: 4.152179469569282e-19\n",
      "iters: 510 cost: 34.678022 --- alpha: 1.2456538408707845e-19\n",
      "iters: 515 cost: 23.542617 --- alpha: 1.2331973024620766e-19\n",
      "iters: 520 cost: 42.023416 --- alpha: 1.2331973024620766e-19\n",
      "iters: 525 cost: 45.032635 --- alpha: 3.662595988312367e-20\n",
      "iters: 530 cost: 14.703396 --- alpha: 1.0987787964937101e-20\n",
      "iters: 535 cost: 5.866512 --- alpha: 3.2633730255863188e-21\n",
      "iters: 540 cost: 3.184362 --- alpha: 3.2633730255863188e-21\n",
      "iters: 545 cost: 25.699277 --- alpha: 9.692217885991366e-22\n",
      "iters: 550 cost: 44.243164 --- alpha: 2.9076653657974096e-22\n",
      "iters: 555 cost: 4.015996 --- alpha: 2.8785887121394353e-22\n",
      "iters: 560 cost: 25.615945 --- alpha: 8.635766136418306e-23\n",
      "iters: 565 cost: 11.288300 --- alpha: 2.5648225425162368e-23\n",
      "iters: 570 cost: 45.507000 --- alpha: 2.5648225425162368e-23\n",
      "iters: 575 cost: 41.550450 --- alpha: 7.617522951273223e-24\n",
      "iters: 580 cost: 2.150436 --- alpha: 2.285256885381967e-24\n",
      "iters: 585 cost: 10.841206 --- alpha: 6.787212949584441e-25\n",
      "iters: 590 cost: 52.747706 --- alpha: 6.787212949584441e-25\n",
      "iters: 595 cost: 2.429093 --- alpha: 2.015802246026579e-25\n",
      "iters: 600 cost: 4.258707 --- alpha: 6.047406738079737e-26\n",
      "iters: 605 cost: 20.259244 --- alpha: 5.986932670698939e-26\n",
      "iters: 610 cost: 34.838469 --- alpha: 5.986932670698939e-26\n",
      "iters: 615 cost: 14.551219 --- alpha: 1.778119003197585e-26\n",
      "iters: 620 cost: 42.952722 --- alpha: 1.778119003197585e-26\n",
      "iters: 625 cost: 60.945394 --- alpha: 1.760337813165609e-26\n",
      "iters: 630 cost: 31.032244 --- alpha: 5.2810134394968265e-27\n",
      "iters: 635 cost: 25.853251 --- alpha: 5.228203305101858e-27\n",
      "iters: 640 cost: 39.353538 --- alpha: 5.228203305101858e-27\n",
      "iters: 645 cost: 54.932864 --- alpha: 1.5527763816152517e-27\n",
      "iters: 650 cost: 37.536236 --- alpha: 1.5527763816152517e-27\n",
      "iters: 655 cost: 12.027324 --- alpha: 1.5372486177990993e-27\n",
      "iters: 660 cost: 44.742556 --- alpha: 1.5372486177990993e-27\n",
      "iters: 665 cost: 12.796875 --- alpha: 4.5656283948633245e-28\n",
      "iters: 670 cost: 45.842115 --- alpha: 4.5656283948633245e-28\n",
      "iters: 675 cost: 2.797083 --- alpha: 4.519972110914691e-28\n",
      "iters: 680 cost: 39.278487 --- alpha: 1.3559916332744072e-28\n",
      "iters: 685 cost: 55.952591 --- alpha: 4.027295150824989e-29\n",
      "iters: 690 cost: 7.979921 --- alpha: 3.62456563574249e-30\n",
      "iters: 695 cost: 39.757868 --- alpha: 1.0764959938155194e-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters: 700 cost: 13.543302 --- alpha: 1.0764959938155194e-30\n",
      "iters: 705 cost: 42.082482 --- alpha: 1.0657310338773642e-30\n",
      "iters: 710 cost: 12.208418 --- alpha: 3.1971931016320926e-31\n",
      "iters: 715 cost: 45.704694 --- alpha: 3.1652211706157717e-31\n",
      "iters: 720 cost: 2.052140 --- alpha: 3.1652211706157717e-31\n",
      "iters: 725 cost: 42.325806 --- alpha: 9.400706876728841e-32\n",
      "iters: 730 cost: 56.692029 --- alpha: 2.8202120630186524e-32\n",
      "iters: 735 cost: 42.667113 --- alpha: 8.376029827165396e-33\n",
      "iters: 740 cost: 2.543604 --- alpha: 2.5128089481496186e-33\n",
      "iters: 745 cost: 18.663498 --- alpha: 2.2389127728013103e-34\n",
      "iters: 750 cost: 46.152632 --- alpha: 6.716738318403931e-35\n",
      "iters: 755 cost: 48.726163 --- alpha: 1.9948712805659673e-35\n",
      "iters: 760 cost: 35.485087 --- alpha: 1.9948712805659673e-35\n",
      "iters: 765 cost: 25.149945 --- alpha: 1.9749225677603076e-35\n",
      "iters: 770 cost: 2.345551 --- alpha: 1.9749225677603076e-35\n",
      "iters: 775 cost: 7.034794 --- alpha: 5.8655200262481134e-36\n",
      "iters: 780 cost: 42.790278 --- alpha: 5.8655200262481134e-36\n",
      "iters: 785 cost: 25.711759 --- alpha: 1.7420594477956896e-36\n",
      "iters: 790 cost: 26.803421 --- alpha: 5.226178343387069e-37\n",
      "iters: 795 cost: 1.997635 --- alpha: 4.656524903957878e-38\n",
      "iters: 800 cost: 42.049446 --- alpha: 4.19087241356209e-39\n",
      "iters: 805 cost: 14.000796 --- alpha: 4.148963689426469e-39\n",
      "iters: 810 cost: 2.555199 --- alpha: 4.148963689426469e-39\n",
      "iters: 815 cost: 43.534575 --- alpha: 1.2322422157596612e-39\n",
      "iters: 820 cost: 41.083164 --- alpha: 3.696726647278984e-40\n",
      "iters: 825 cost: 47.107677 --- alpha: 1.0979278142418582e-40\n",
      "iters: 830 cost: 40.201235 --- alpha: 3.2937834427255747e-41\n",
      "iters: 835 cost: 27.656201 --- alpha: 9.782536824894956e-42\n",
      "iters: 840 cost: 2.591873 --- alpha: 9.782536824894956e-42\n",
      "iters: 845 cost: 9.509597 --- alpha: 2.905413436993802e-42\n",
      "iters: 850 cost: 43.762564 --- alpha: 2.905413436993802e-42\n",
      "iters: 855 cost: 43.416399 --- alpha: 2.876359302623864e-42\n",
      "iters: 860 cost: 6.153920 --- alpha: 2.876359302623864e-42\n",
      "iters: 865 cost: 2.167094 --- alpha: 8.542787128792876e-43\n",
      "iters: 870 cost: 35.159753 --- alpha: 2.5628361386378626e-43\n",
      "iters: 875 cost: 34.391039 --- alpha: 2.537207777251484e-43\n",
      "iters: 880 cost: 27.895310 --- alpha: 2.537207777251484e-43\n",
      "iters: 885 cost: 40.782940 --- alpha: 2.511835699478969e-43\n",
      "iters: 890 cost: 17.450928 --- alpha: 2.511835699478969e-43\n",
      "iters: 895 cost: 6.385844 --- alpha: 7.460152027452536e-44\n",
      "iters: 900 cost: 34.081303 --- alpha: 7.460152027452536e-44\n",
      "iters: 905 cost: 22.152208 --- alpha: 7.385550507178011e-44\n",
      "iters: 910 cost: 45.546546 --- alpha: 2.2156651521534033e-44\n",
      "iters: 915 cost: 2.319633 --- alpha: 6.580525501895608e-45\n",
      "iters: 920 cost: 67.912119 --- alpha: 5.9224729517060465e-46\n",
      "iters: 925 cost: 49.441575 --- alpha: 5.863248222188986e-46\n",
      "iters: 930 cost: 64.812800 --- alpha: 5.863248222188986e-46\n",
      "iters: 935 cost: 42.667727 --- alpha: 1.7413847219901288e-46\n",
      "iters: 940 cost: 2.733943 --- alpha: 1.7413847219901288e-46\n",
      "iters: 945 cost: 27.650609 --- alpha: 5.171912624310682e-47\n",
      "iters: 950 cost: 57.774651 --- alpha: 5.171912624310682e-47\n",
      "iters: 955 cost: 31.766334 --- alpha: 1.5360580494202725e-47\n",
      "iters: 960 cost: 2.968569 --- alpha: 1.5360580494202725e-47\n",
      "iters: 965 cost: 2.671642 --- alpha: 1.3686277220334626e-48\n",
      "iters: 970 cost: 34.361557 --- alpha: 1.2317649498301161e-49\n",
      "iters: 975 cost: 1.883866 --- alpha: 1.2194473003318149e-49\n",
      "iters: 980 cost: 21.540687 --- alpha: 1.0975025702986332e-50\n",
      "iters: 985 cost: 1.980442 --- alpha: 3.2595826337869407e-51\n",
      "iters: 990 cost: 20.551536 --- alpha: 2.9336243704082465e-52\n",
      "iters: 995 cost: 55.265234 --- alpha: 8.712864380112492e-53\n",
      "iters: 1000 cost: 6.414619 --- alpha: 2.6138593140337475e-53\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "data = get_text(path)\n",
    "data = tokenize(data)\n",
    "# C = 10\n",
    "# batch_size = 2048\n",
    "# alpha = 0.001\n",
    "N = 300\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1_1, W1_2, W1_3, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c72b9b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 5 cost: 48.713220 --- alpha: 0.03\n",
      "iters: 10 cost: 105.856746 --- alpha: 0.03\n",
      "iters: 15 cost: 114.976174 --- alpha: 0.03\n",
      "iters: 20 cost: 45.294302 --- alpha: 0.03\n",
      "iters: 25 cost: 34.625680 --- alpha: 0.03\n",
      "iters: 30 cost: 58.594012 --- alpha: 0.03\n",
      "iters: 35 cost: 49.256010 --- alpha: 0.03\n",
      "iters: 40 cost: 41.570173 --- alpha: 0.03\n",
      "iters: 45 cost: 24.658628 --- alpha: 0.03\n",
      "iters: 50 cost: 52.937060 --- alpha: 0.03\n",
      "iters: 55 cost: 22.890490 --- alpha: 0.0225\n",
      "iters: 60 cost: 25.514921 --- alpha: 0.0225\n",
      "iters: 65 cost: 35.952617 --- alpha: 0.0225\n",
      "iters: 70 cost: 41.748288 --- alpha: 0.0225\n",
      "iters: 75 cost: 55.671922 --- alpha: 0.0225\n",
      "iters: 80 cost: 35.890737 --- alpha: 0.0225\n",
      "iters: 85 cost: 39.034263 --- alpha: 0.0225\n",
      "iters: 90 cost: 27.163401 --- alpha: 0.0225\n",
      "iters: 95 cost: 36.001190 --- alpha: 0.0225\n",
      "iters: 100 cost: 39.690770 --- alpha: 0.0225\n",
      "iters: 105 cost: 7.271681 --- alpha: 0.012656249999999999\n",
      "iters: 110 cost: 16.306599 --- alpha: 0.012656249999999999\n",
      "iters: 115 cost: 27.917217 --- alpha: 0.012656249999999999\n",
      "iters: 120 cost: 21.681964 --- alpha: 0.012656249999999999\n",
      "iters: 125 cost: 30.595296 --- alpha: 0.012656249999999999\n",
      "iters: 130 cost: 33.509102 --- alpha: 0.012656249999999999\n",
      "iters: 135 cost: 6.925636 --- alpha: 0.012656249999999999\n",
      "iters: 140 cost: 6.651017 --- alpha: 0.012656249999999999\n",
      "iters: 145 cost: 25.112143 --- alpha: 0.012656249999999999\n",
      "iters: 150 cost: 13.856587 --- alpha: 0.012656249999999999\n",
      "iters: 155 cost: 4.774334 --- alpha: 0.005339355468749999\n",
      "iters: 160 cost: 12.340218 --- alpha: 0.005339355468749999\n",
      "iters: 165 cost: 11.535442 --- alpha: 0.005339355468749999\n",
      "iters: 170 cost: 17.877621 --- alpha: 0.005339355468749999\n",
      "iters: 175 cost: 12.174278 --- alpha: 0.005339355468749999\n",
      "iters: 180 cost: 14.871958 --- alpha: 0.005339355468749999\n",
      "iters: 185 cost: 14.693265 --- alpha: 0.005339355468749999\n",
      "iters: 190 cost: 19.333168 --- alpha: 0.005339355468749999\n",
      "iters: 195 cost: 19.759711 --- alpha: 0.005339355468749999\n",
      "iters: 200 cost: 16.439330 --- alpha: 0.005339355468749999\n",
      "iters: 205 cost: 28.092570 --- alpha: 0.0016894054412841795\n",
      "iters: 210 cost: 22.302501 --- alpha: 0.0016894054412841795\n",
      "iters: 215 cost: 23.687514 --- alpha: 0.0016894054412841795\n",
      "iters: 220 cost: 14.475616 --- alpha: 0.0016894054412841795\n",
      "iters: 225 cost: 29.073897 --- alpha: 0.0016894054412841795\n",
      "iters: 230 cost: 7.263952 --- alpha: 0.0016894054412841795\n",
      "iters: 235 cost: 11.503532 --- alpha: 0.0016894054412841795\n",
      "iters: 240 cost: 4.219066 --- alpha: 0.0016894054412841795\n",
      "iters: 245 cost: 14.731733 --- alpha: 0.0016894054412841795\n",
      "iters: 250 cost: 14.832564 --- alpha: 0.0016894054412841795\n",
      "iters: 255 cost: 4.364404 --- alpha: 0.0004009038303047418\n",
      "iters: 260 cost: 22.460072 --- alpha: 0.0004009038303047418\n",
      "iters: 265 cost: 6.035298 --- alpha: 0.0004009038303047418\n",
      "iters: 270 cost: 30.522337 --- alpha: 0.0004009038303047418\n",
      "iters: 275 cost: 6.265783 --- alpha: 0.0004009038303047418\n",
      "iters: 280 cost: 5.382436 --- alpha: 0.0004009038303047418\n",
      "iters: 285 cost: 15.642134 --- alpha: 0.0004009038303047418\n",
      "iters: 290 cost: 25.149535 --- alpha: 0.0004009038303047418\n",
      "iters: 295 cost: 12.759140 --- alpha: 0.0004009038303047418\n",
      "iters: 300 cost: 11.826446 --- alpha: 0.0004009038303047418\n",
      "iters: 305 cost: 22.536762 --- alpha: 7.135226862601484e-05\n",
      "iters: 310 cost: 4.628308 --- alpha: 7.135226862601484e-05\n",
      "iters: 315 cost: 7.836822 --- alpha: 7.135226862601484e-05\n",
      "iters: 320 cost: 12.967710 --- alpha: 7.135226862601484e-05\n",
      "iters: 325 cost: 21.540228 --- alpha: 7.135226862601484e-05\n",
      "iters: 330 cost: 12.694569 --- alpha: 7.135226862601484e-05\n",
      "iters: 335 cost: 6.391084 --- alpha: 7.135226862601484e-05\n",
      "iters: 340 cost: 26.036480 --- alpha: 7.135226862601484e-05\n",
      "iters: 345 cost: 26.926271 --- alpha: 7.135226862601484e-05\n",
      "iters: 350 cost: 25.734737 --- alpha: 7.135226862601484e-05\n",
      "iters: 355 cost: 25.675084 --- alpha: 9.524378142400785e-06\n",
      "iters: 360 cost: 22.095897 --- alpha: 9.524378142400785e-06\n",
      "iters: 365 cost: 21.245363 --- alpha: 9.524378142400785e-06\n",
      "iters: 370 cost: 19.859022 --- alpha: 9.524378142400785e-06\n",
      "iters: 375 cost: 4.524758 --- alpha: 9.524378142400785e-06\n",
      "iters: 380 cost: 16.683835 --- alpha: 9.524378142400785e-06\n",
      "iters: 385 cost: 4.030701 --- alpha: 9.524378142400785e-06\n",
      "iters: 390 cost: 16.566430 --- alpha: 9.524378142400785e-06\n",
      "iters: 395 cost: 21.404506 --- alpha: 9.524378142400785e-06\n",
      "iters: 400 cost: 17.185211 --- alpha: 9.524378142400785e-06\n",
      "iters: 405 cost: 6.416409 --- alpha: 9.535132597700737e-07\n",
      "iters: 410 cost: 11.929434 --- alpha: 9.535132597700737e-07\n",
      "iters: 415 cost: 13.243480 --- alpha: 9.535132597700737e-07\n",
      "iters: 420 cost: 1.838551 --- alpha: 9.535132597700737e-07\n",
      "iters: 425 cost: 19.743026 --- alpha: 9.535132597700737e-07\n",
      "iters: 430 cost: 13.529610 --- alpha: 9.535132597700737e-07\n",
      "iters: 435 cost: 31.753078 --- alpha: 9.535132597700737e-07\n",
      "iters: 440 cost: 13.979955 --- alpha: 9.535132597700737e-07\n",
      "iters: 445 cost: 12.691362 --- alpha: 9.535132597700737e-07\n",
      "iters: 450 cost: 20.915588 --- alpha: 9.535132597700737e-07\n",
      "iters: 455 cost: 2.537208 --- alpha: 7.15942439729857e-08\n",
      "iters: 460 cost: 10.715786 --- alpha: 7.15942439729857e-08\n",
      "iters: 465 cost: 13.103702 --- alpha: 7.15942439729857e-08\n",
      "iters: 470 cost: 6.459132 --- alpha: 7.15942439729857e-08\n",
      "iters: 475 cost: 15.086370 --- alpha: 7.15942439729857e-08\n",
      "iters: 480 cost: 6.783376 --- alpha: 7.15942439729857e-08\n",
      "iters: 485 cost: 3.697396 --- alpha: 7.15942439729857e-08\n",
      "iters: 490 cost: 17.299349 --- alpha: 7.15942439729857e-08\n",
      "iters: 495 cost: 4.601350 --- alpha: 7.15942439729857e-08\n",
      "iters: 500 cost: 13.401103 --- alpha: 7.15942439729857e-08\n",
      "iters: 505 cost: 3.900617 --- alpha: 4.031723511086304e-09\n",
      "iters: 510 cost: 14.719426 --- alpha: 4.031723511086304e-09\n",
      "iters: 515 cost: 15.547259 --- alpha: 4.031723511086304e-09\n",
      "iters: 520 cost: 3.671421 --- alpha: 4.031723511086304e-09\n",
      "iters: 525 cost: 9.192074 --- alpha: 4.031723511086304e-09\n",
      "iters: 530 cost: 11.058051 --- alpha: 4.031723511086304e-09\n",
      "iters: 535 cost: 7.493092 --- alpha: 4.031723511086304e-09\n",
      "iters: 540 cost: 3.923069 --- alpha: 4.031723511086304e-09\n",
      "iters: 545 cost: 17.081874 --- alpha: 4.031723511086304e-09\n",
      "iters: 550 cost: 19.507553 --- alpha: 4.031723511086304e-09\n",
      "iters: 555 cost: 4.879902 --- alpha: 1.7028039093456402e-10\n",
      "iters: 560 cost: 7.591058 --- alpha: 1.7028039093456402e-10\n",
      "iters: 565 cost: 6.674389 --- alpha: 1.7028039093456402e-10\n",
      "iters: 570 cost: 19.581797 --- alpha: 1.7028039093456402e-10\n",
      "iters: 575 cost: 3.809043 --- alpha: 1.7028039093456402e-10\n",
      "iters: 580 cost: 2.888510 --- alpha: 1.7028039093456402e-10\n",
      "iters: 585 cost: 11.477504 --- alpha: 1.7028039093456402e-10\n",
      "iters: 590 cost: 4.885621 --- alpha: 1.7028039093456402e-10\n",
      "iters: 595 cost: 2.880416 --- alpha: 1.7028039093456402e-10\n",
      "iters: 600 cost: 5.854920 --- alpha: 1.7028039093456402e-10\n",
      "iters: 605 cost: 16.899338 --- alpha: 5.393861606040933e-12\n",
      "iters: 610 cost: 20.131064 --- alpha: 5.393861606040933e-12\n",
      "iters: 615 cost: 10.078628 --- alpha: 5.393861606040933e-12\n",
      "iters: 620 cost: 14.159960 --- alpha: 5.393861606040933e-12\n",
      "iters: 625 cost: 12.671273 --- alpha: 5.393861606040933e-12\n",
      "iters: 630 cost: 15.992580 --- alpha: 5.393861606040933e-12\n",
      "iters: 635 cost: 6.864450 --- alpha: 5.393861606040933e-12\n",
      "iters: 640 cost: 17.837731 --- alpha: 5.393861606040933e-12\n",
      "iters: 645 cost: 28.039841 --- alpha: 5.393861606040933e-12\n",
      "iters: 650 cost: 17.026728 --- alpha: 5.393861606040933e-12\n",
      "iters: 655 cost: 5.673328 --- alpha: 1.28143394251585e-13\n",
      "iters: 660 cost: 26.936513 --- alpha: 1.28143394251585e-13\n",
      "iters: 665 cost: 6.209341 --- alpha: 1.28143394251585e-13\n",
      "iters: 670 cost: 6.837710 --- alpha: 1.28143394251585e-13\n",
      "iters: 675 cost: 3.227842 --- alpha: 1.28143394251585e-13\n",
      "iters: 680 cost: 15.385499 --- alpha: 1.28143394251585e-13\n",
      "iters: 685 cost: 25.206334 --- alpha: 1.28143394251585e-13\n",
      "iters: 690 cost: 5.975639 --- alpha: 1.28143394251585e-13\n",
      "iters: 695 cost: 15.391231 --- alpha: 1.28143394251585e-13\n",
      "iters: 700 cost: 8.648115 --- alpha: 1.28143394251585e-13\n",
      "iters: 705 cost: 17.814174 --- alpha: 2.283252337053825e-15\n",
      "iters: 710 cost: 6.279302 --- alpha: 2.283252337053825e-15\n",
      "iters: 715 cost: 23.439678 --- alpha: 2.283252337053825e-15\n",
      "iters: 720 cost: 2.754094 --- alpha: 2.283252337053825e-15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters: 725 cost: 3.996875 --- alpha: 2.283252337053825e-15\n",
      "iters: 730 cost: 27.337592 --- alpha: 2.283252337053825e-15\n",
      "iters: 735 cost: 21.865791 --- alpha: 2.283252337053825e-15\n",
      "iters: 740 cost: 3.557248 --- alpha: 2.283252337053825e-15\n",
      "iters: 745 cost: 6.882426 --- alpha: 2.283252337053825e-15\n",
      "iters: 750 cost: 19.710362 --- alpha: 2.283252337053825e-15\n",
      "iters: 755 cost: 26.153078 --- alpha: 3.0512153582571067e-17\n",
      "iters: 760 cost: 12.947917 --- alpha: 3.0512153582571067e-17\n",
      "iters: 765 cost: 13.579364 --- alpha: 3.0512153582571067e-17\n",
      "iters: 770 cost: 2.753292 --- alpha: 3.0512153582571067e-17\n",
      "iters: 775 cost: 4.913604 --- alpha: 3.0512153582571067e-17\n",
      "iters: 780 cost: 4.280865 --- alpha: 3.0512153582571067e-17\n",
      "iters: 785 cost: 7.716035 --- alpha: 3.0512153582571067e-17\n",
      "iters: 790 cost: 15.948627 --- alpha: 3.0512153582571067e-17\n",
      "iters: 795 cost: 2.664316 --- alpha: 3.0512153582571067e-17\n",
      "iters: 800 cost: 19.052697 --- alpha: 3.0512153582571067e-17\n",
      "iters: 805 cost: 4.947214 --- alpha: 3.058109810524823e-19\n",
      "iters: 810 cost: 3.052870 --- alpha: 3.058109810524823e-19\n",
      "iters: 815 cost: 18.994855 --- alpha: 3.058109810524823e-19\n",
      "iters: 820 cost: 18.901217 --- alpha: 3.058109810524823e-19\n",
      "iters: 825 cost: 20.848929 --- alpha: 3.058109810524823e-19\n",
      "iters: 830 cost: 21.627348 --- alpha: 3.058109810524823e-19\n",
      "iters: 835 cost: 4.717904 --- alpha: 3.058109810524823e-19\n",
      "iters: 840 cost: 2.882860 --- alpha: 3.058109810524823e-19\n",
      "iters: 845 cost: 8.580557 --- alpha: 3.058109810524823e-19\n",
      "iters: 850 cost: 5.295233 --- alpha: 3.058109810524823e-19\n",
      "iters: 855 cost: 21.432231 --- alpha: 2.298764880997331e-21\n",
      "iters: 860 cost: 4.218383 --- alpha: 2.298764880997331e-21\n",
      "iters: 865 cost: 2.653521 --- alpha: 2.298764880997331e-21\n",
      "iters: 870 cost: 11.718337 --- alpha: 2.298764880997331e-21\n",
      "iters: 875 cost: 19.863213 --- alpha: 2.298764880997331e-21\n",
      "iters: 880 cost: 4.558448 --- alpha: 2.298764880997331e-21\n",
      "iters: 885 cost: 20.643695 --- alpha: 2.298764880997331e-21\n",
      "iters: 890 cost: 10.071415 --- alpha: 2.298764880997331e-21\n",
      "iters: 895 cost: 7.740756 --- alpha: 2.298764880997331e-21\n",
      "iters: 900 cost: 15.912737 --- alpha: 2.298764880997331e-21\n",
      "iters: 905 cost: 11.290735 --- alpha: 1.2959770018526074e-23\n",
      "iters: 910 cost: 6.955032 --- alpha: 1.2959770018526074e-23\n",
      "iters: 915 cost: 3.236665 --- alpha: 1.2959770018526074e-23\n",
      "iters: 920 cost: 29.076674 --- alpha: 1.2959770018526074e-23\n",
      "iters: 925 cost: 19.872642 --- alpha: 1.2959770018526074e-23\n",
      "iters: 930 cost: 31.892860 --- alpha: 1.2959770018526074e-23\n",
      "iters: 935 cost: 7.259636 --- alpha: 1.2959770018526074e-23\n",
      "iters: 940 cost: 3.385737 --- alpha: 1.2959770018526074e-23\n",
      "iters: 945 cost: 4.575665 --- alpha: 1.2959770018526074e-23\n",
      "iters: 950 cost: 30.339918 --- alpha: 1.2959770018526074e-23\n",
      "iters: 955 cost: 17.481964 --- alpha: 5.479756987811827e-26\n",
      "iters: 960 cost: 3.575406 --- alpha: 5.479756987811827e-26\n",
      "iters: 965 cost: 3.456476 --- alpha: 5.479756987811827e-26\n",
      "iters: 970 cost: 16.440199 --- alpha: 5.479756987811827e-26\n",
      "iters: 975 cost: 2.803803 --- alpha: 5.479756987811827e-26\n",
      "iters: 980 cost: 6.566589 --- alpha: 5.479756987811827e-26\n",
      "iters: 985 cost: 2.975648 --- alpha: 5.479756987811827e-26\n",
      "iters: 990 cost: 5.678586 --- alpha: 5.479756987811827e-26\n",
      "iters: 995 cost: 7.047785 --- alpha: 5.479756987811827e-26\n",
      "iters: 1000 cost: 4.424706 --- alpha: 5.479756987811827e-26\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "data = get_text(path)\n",
    "data = tokenize(data)\n",
    "# C = 10\n",
    "# batch_size = 512\n",
    "# alpha = 0.03\n",
    "N = 300\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1_1, W1_2, W1_3, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cc69b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract word vectors we have three approaches\n",
    "# First approach: to assume W1 as word vectors \n",
    "# embs = W1.T\n",
    "\n",
    "# Second approach: to assume W2 as word vectors \n",
    "# embs = W2\n",
    "\n",
    "# Third approach: or to assume the mean of W1+W2 as word vectors \n",
    "embs = (W1_1.T + W1_2.T + W1_3.T + W2)/2.0\n",
    "\n",
    "word_embeddings = {}\n",
    "for i, vec in enumerate(embs):\n",
    "    word_embeddings[Ind2word[i]] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75434b8",
   "metadata": {},
   "source": [
    "# Visualizing the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaedd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 2 libraries for dispalying arabic text on a plot properly\n",
    "# pip install python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bed139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade arabic-reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee149fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 300) [14368, 12083, 2181, 5819, 18371, 11727, 18445, 3142, 3086, 17326]\n"
     ]
    }
   ],
   "source": [
    "# visualizing the word vectors here\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import arabic_reshaper\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['قال', 'صراط','الآية','المستقيم', 'هذا','سياق','واحدة',\n",
    "         'التسهيل','التخفيف','معدودة']\n",
    "\n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce1be947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"377.927131pt\" height=\"248.518125pt\" viewBox=\"0 0 377.927131 248.518125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2022-05-25T07:33:21.889805</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 248.518125 \n",
       "L 377.927131 248.518125 \n",
       "L 377.927131 0 \n",
       "L 0 0 \n",
       "L 0 248.518125 \n",
       "z\n",
       "\" style=\"fill: none\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 28.942188 224.64 \n",
       "L 363.742188 224.64 \n",
       "L 363.742188 7.2 \n",
       "L 28.942188 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"PathCollection_1\">\n",
       "    <defs>\n",
       "     <path id=\"m811dc46505\" d=\"M 0 3 \n",
       "C 0.795609 3 1.55874 2.683901 2.12132 2.12132 \n",
       "C 2.683901 1.55874 3 0.795609 3 0 \n",
       "C 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \n",
       "C 1.55874 -2.683901 0.795609 -3 0 -3 \n",
       "C -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \n",
       "C -2.683901 -1.55874 -3 -0.795609 -3 0 \n",
       "C -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \n",
       "C -1.55874 2.683901 -0.795609 3 0 3 \n",
       "z\n",
       "\" style=\"stroke: #1f77b4\"/>\n",
       "    </defs>\n",
       "    <g clip-path=\"url(#p0ef57a2678)\">\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"316.174143\" y=\"186.215969\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"205.927274\" y=\"130.705404\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"196.664542\" y=\"70.447612\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"102.889919\" y=\"32.227355\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"49.458797\" y=\"60.130087\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"348.524006\" y=\"80.931509\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"217.01323\" y=\"23.183692\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"285.877512\" y=\"124.044463\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"44.160369\" y=\"214.756364\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "     <use xlink:href=\"#m811dc46505\" x=\"235.396514\" y=\"17.083636\" style=\"fill: #1f77b4; stroke: #1f77b4\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"mfdcc0d8f44\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdcc0d8f44\" x=\"81.204375\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(73.833281 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2212\" d=\"M 678 2272 \n",
       "L 4684 2272 \n",
       "L 4684 1741 \n",
       "L 678 1741 \n",
       "L 678 2272 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdcc0d8f44\" x=\"140.706503\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(133.335409 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdcc0d8f44\" x=\"200.208631\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(197.027381 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdcc0d8f44\" x=\"259.710758\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(256.529508 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#mfdcc0d8f44\" x=\"319.212886\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(316.031636 239.238437)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m6284520599\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"207.367611\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- −3 -->\n",
       "      <g transform=\"translate(7.2 211.16683)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"169.569277\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- −2 -->\n",
       "      <g transform=\"translate(7.2 173.368496)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"131.770943\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- −1 -->\n",
       "      <g transform=\"translate(7.2 135.570162)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-2212\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"83.789062\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"93.972609\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(15.579688 97.771828)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"56.174275\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(15.579688 59.973494)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6284520599\" x=\"28.942188\" y=\"18.375941\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(15.579688 22.17516)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 28.942188 224.64 \n",
       "L 28.942188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 363.742188 224.64 \n",
       "L 363.742188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 28.942187 224.64 \n",
       "L 363.742188 224.64 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 28.942187 7.2 \n",
       "L 363.742188 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"text_12\">\n",
       "    <!-- ﻝﺎﻗ -->\n",
       "    <g transform=\"translate(316.174143 186.215969)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fedd\" d=\"M 3256 228 \n",
       "Q 3500 622 3500 1331 \n",
       "L 3500 4863 \n",
       "L 4075 4863 \n",
       "L 4075 1331 \n",
       "Q 4075 341 3769 -109 \n",
       "Q 3391 -663 2575 -869 \n",
       "Q 2156 -975 1891 -975 \n",
       "Q 1594 -975 1366 -906 \n",
       "Q 453 -619 450 234 \n",
       "Q 447 666 647 953 \n",
       "L 1222 953 \n",
       "Q 1019 594 1019 234 \n",
       "Q 1019 -172 1538 -366 \n",
       "Q 1663 -416 1891 -416 \n",
       "Q 2141 -416 2481 -309 \n",
       "Q 3025 -144 3256 228 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fe8e\" d=\"M 603 1159 \n",
       "L 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 1197 \n",
       "Q 1178 866 1331 713 \n",
       "Q 1469 575 1778 575 \n",
       "L 2013 575 \n",
       "L 2013 0 \n",
       "L 1606 0 \n",
       "Q 1125 0 875 288 \n",
       "Q 603 603 603 1159 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fed7\" d=\"M 1869 1678 \n",
       "Q 1997 1813 1997 2053 \n",
       "Q 1997 2238 1791 2397 \n",
       "Q 1694 2475 1556 2472 \n",
       "Q 1394 2469 1269 2338 \n",
       "Q 1144 2209 1141 2050 \n",
       "Q 1141 1800 1313 1681 \n",
       "Q 1438 1603 1569 1603 \n",
       "Q 1794 1603 1869 1678 \n",
       "z\n",
       "M -63 575 \n",
       "L 691 575 \n",
       "Q 953 575 1425 647 \n",
       "Q 1672 684 1859 975 \n",
       "Q 1956 1125 2003 1281 \n",
       "Q 1806 1109 1503 1103 \n",
       "Q 1097 1097 853 1313 \n",
       "Q 547 1584 547 1988 \n",
       "Q 547 2150 572 2288 \n",
       "Q 644 2750 1113 2966 \n",
       "Q 1356 3078 1594 3078 \n",
       "Q 1875 3078 2081 2922 \n",
       "Q 2381 2697 2525 2378 \n",
       "Q 2597 2219 2597 1781 \n",
       "Q 2597 1131 2353 684 \n",
       "Q 2122 259 1734 122 \n",
       "Q 1394 0 941 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "z\n",
       "M 1717 4053 \n",
       "L 2186 4053 \n",
       "L 2186 3584 \n",
       "L 1717 3584 \n",
       "L 1717 4053 \n",
       "z\n",
       "M 936 4053 \n",
       "L 1405 4053 \n",
       "L 1405 3584 \n",
       "L 936 3584 \n",
       "L 936 4053 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fedd\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8e\" x=\"72.65625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fed7\" x=\"103.125\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_13\">\n",
       "    <!-- ﻁﺍﺮﺻ -->\n",
       "    <g transform=\"translate(205.927274 130.705404)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fec1\" d=\"M 2603 575 \n",
       "L 3188 575 \n",
       "Q 3928 575 4331 772 \n",
       "Q 4916 1063 4913 1275 \n",
       "Q 4906 1697 4478 1756 \n",
       "Q 4363 1772 4213 1772 \n",
       "Q 3963 1772 3581 1519 \n",
       "Q 3028 1144 2603 575 \n",
       "z\n",
       "M 3394 0 \n",
       "L 1597 0 \n",
       "L 450 0 \n",
       "L 450 575 \n",
       "L 1597 575 \n",
       "L 1597 4863 \n",
       "L 2172 4863 \n",
       "L 2172 906 \n",
       "Q 2850 1903 3713 2213 \n",
       "Q 4000 2316 4213 2316 \n",
       "Q 4588 2316 4906 2194 \n",
       "Q 5488 1978 5488 1244 \n",
       "Q 5488 716 4913 375 \n",
       "Q 4281 0 3394 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fe8d\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feae\" d=\"M 1975 1719 \n",
       "L 2550 1719 \n",
       "Q 2644 1369 2653 1275 \n",
       "Q 2684 953 2913 713 \n",
       "Q 3044 575 3359 575 \n",
       "L 3594 575 \n",
       "L 3594 0 \n",
       "L 3188 0 \n",
       "Q 2781 0 2625 194 \n",
       "Q 2413 -675 1628 -1091 \n",
       "Q 741 -1563 -266 -1563 \n",
       "L -266 -988 \n",
       "Q 684 -988 1319 -588 \n",
       "Q 2000 -156 2109 494 \n",
       "Q 2141 675 2141 900 \n",
       "Q 2141 1294 1975 1719 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-febb\" d=\"M 3050 1519 \n",
       "Q 2525 1181 2072 575 \n",
       "L 2656 575 \n",
       "Q 3397 575 3800 772 \n",
       "Q 4381 1056 4381 1275 \n",
       "Q 4381 1697 3947 1756 \n",
       "Q 3831 1772 3681 1772 \n",
       "Q 3444 1772 3050 1519 \n",
       "z\n",
       "M 2863 0 \n",
       "L 2000 0 \n",
       "Q 1769 0 1525 131 \n",
       "Q 1322 241 1200 431 \n",
       "Q 881 0 344 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 172 575 \n",
       "Q 481 575 619 713 \n",
       "Q 869 963 869 1275 \n",
       "L 869 1631 \n",
       "L 1444 1631 \n",
       "L 1444 1388 \n",
       "Q 1444 1313 1472 1188 \n",
       "Q 1500 1063 1641 906 \n",
       "Q 1956 1325 2253 1588 \n",
       "Q 2731 2016 3181 2213 \n",
       "Q 3416 2316 3681 2316 \n",
       "Q 4041 2316 4375 2194 \n",
       "Q 4956 1981 4956 1244 \n",
       "Q 4956 716 4381 375 \n",
       "Q 3750 0 2863 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fec1\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"92.480469\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feae\" x=\"120.263672\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-febb\" x=\"175.439453\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_14\">\n",
       "    <!-- ﺔﻳﻵﺍ -->\n",
       "    <g transform=\"translate(196.664542 70.447612)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fe94\" d=\"M 2113 881 \n",
       "Q 2066 1013 2025 1166 \n",
       "Q 1984 1322 1947 1638 \n",
       "Q 1572 1600 1316 1391 \n",
       "Q 969 1113 975 919 \n",
       "Q 978 791 1259 712 \n",
       "Q 1541 634 2113 881 \n",
       "z\n",
       "M 1903 2088 \n",
       "Q 1891 2209 1881 2344 \n",
       "L 2456 2344 \n",
       "Q 2459 1819 2572 1275 \n",
       "Q 2638 959 2813 713 \n",
       "Q 2909 575 3259 575 \n",
       "L 3494 575 \n",
       "L 3494 0 \n",
       "L 3088 0 \n",
       "Q 2806 0 2578 159 \n",
       "Q 2438 256 2319 441 \n",
       "Q 1888 213 1400 213 \n",
       "Q 1225 213 1044 259 \n",
       "Q 453 409 453 891 \n",
       "Q 453 1431 1091 1828 \n",
       "Q 1431 2041 1903 2088 \n",
       "z\n",
       "M 1877 3285 \n",
       "L 2346 3285 \n",
       "L 2346 2816 \n",
       "L 1877 2816 \n",
       "L 1877 3285 \n",
       "z\n",
       "M 1096 3285 \n",
       "L 1565 3285 \n",
       "L 1565 2816 \n",
       "L 1096 2816 \n",
       "L 1096 3285 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fef3\" d=\"M 934 269 \n",
       "Q 709 0 219 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 47 575 \n",
       "Q 356 575 494 713 \n",
       "Q 647 866 647 1197 \n",
       "L 647 1875 \n",
       "L 1222 1875 \n",
       "L 1222 1197 \n",
       "Q 1222 613 934 269 \n",
       "z\n",
       "M 1093 -491 \n",
       "L 1562 -491 \n",
       "L 1562 -960 \n",
       "L 1093 -960 \n",
       "L 1093 -491 \n",
       "z\n",
       "M 312 -491 \n",
       "L 781 -491 \n",
       "L 781 -960 \n",
       "L 312 -960 \n",
       "L 312 -491 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fef5\" d=\"M 2997 2606 \n",
       "Q 2997 1669 2563 988 \n",
       "Q 2391 722 2150 500 \n",
       "Q 1509 -63 1016 -63 \n",
       "Q 684 -63 450 38 \n",
       "L 450 613 \n",
       "Q 750 513 1016 513 \n",
       "Q 1369 513 1794 988 \n",
       "L 263 4406 \n",
       "L 822 4406 \n",
       "L 2153 1409 \n",
       "Q 2244 1544 2306 1716 \n",
       "Q 2422 2144 2422 2719 \n",
       "L 2422 4863 \n",
       "L 2997 4863 \n",
       "L 2997 2606 \n",
       "z\n",
       "M -653 5129 \n",
       "L -153 5513 \n",
       "Q 169 5254 378 5188 \n",
       "Q 510 5148 678 5148 \n",
       "Q 860 5148 988 5198 \n",
       "Q 1322 5326 1597 5563 \n",
       "L 1597 5173 \n",
       "Q 1316 4973 1035 4882 \n",
       "Q 825 4813 660 4813 \n",
       "Q 553 4813 316 4866 \n",
       "Q 75 4919 -137 5138 \n",
       "L -653 4738 \n",
       "L -653 5129 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fe94\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef3\" x=\"53.613281\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef5\" x=\"81.445312\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"138.476562\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_15\">\n",
       "    <!-- ﻢﻴﻘﺘﺴﻤﻟﺍ -->\n",
       "    <g transform=\"translate(102.889919 32.227355)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fee2\" d=\"M 1906 525 \n",
       "Q 2288 394 2525 394 \n",
       "Q 2663 394 2728 463 \n",
       "Q 2894 641 2894 822 \n",
       "Q 2894 897 2875 959 \n",
       "Q 2809 1234 2625 1288 \n",
       "Q 2500 1325 2356 1325 \n",
       "Q 2172 1325 2088 1241 \n",
       "Q 1875 1028 1875 816 \n",
       "Q 1875 691 1906 525 \n",
       "z\n",
       "M 3275 163 \n",
       "Q 3228 119 3188 94 \n",
       "Q 2772 -156 2538 -156 \n",
       "Q 1972 -156 1531 88 \n",
       "Q 1375 175 1141 -69 \n",
       "Q 1063 -150 1063 -397 \n",
       "L 1063 -1538 \n",
       "L 438 -1538 \n",
       "L 438 -397 \n",
       "Q 438 144 813 463 \n",
       "Q 1016 634 1297 634 \n",
       "Q 1291 766 1291 891 \n",
       "Q 1291 1353 1797 1766 \n",
       "Q 2038 1963 2325 1963 \n",
       "Q 2550 1963 2813 1850 \n",
       "Q 3344 1628 3469 1069 \n",
       "Q 3516 859 3641 722 \n",
       "Q 3759 584 4088 584 \n",
       "L 4322 584 \n",
       "L 4322 9 \n",
       "L 3916 9 \n",
       "Q 3369 9 3275 163 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fef4\" d=\"M 934 269 \n",
       "Q 694 0 219 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 47 575 \n",
       "Q 356 575 494 713 \n",
       "Q 647 866 647 1197 \n",
       "L 647 1875 \n",
       "L 1222 1875 \n",
       "L 1222 1197 \n",
       "Q 1222 866 1375 713 \n",
       "Q 1513 575 1822 575 \n",
       "L 1994 575 \n",
       "L 1994 0 \n",
       "L 1650 0 \n",
       "Q 1181 0 934 269 \n",
       "z\n",
       "M 1093 -491 \n",
       "L 1562 -491 \n",
       "L 1562 -960 \n",
       "L 1093 -960 \n",
       "L 1093 -491 \n",
       "z\n",
       "M 312 -491 \n",
       "L 781 -491 \n",
       "L 781 -960 \n",
       "L 312 -960 \n",
       "L 312 -491 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fed8\" d=\"M 1919 2381 \n",
       "Q 2491 2131 2566 1663 \n",
       "Q 2591 1506 2591 1375 \n",
       "Q 2591 1147 2525 994 \n",
       "Q 2434 784 2322 628 \n",
       "Q 2444 575 2903 575 \n",
       "L 3300 575 \n",
       "L 3300 0 \n",
       "L 2622 0 \n",
       "Q 2197 0 1619 178 \n",
       "Q 1041 0 616 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 334 575 \n",
       "Q 794 575 916 628 \n",
       "Q 825 753 713 994 \n",
       "Q 647 1138 647 1375 \n",
       "Q 647 1513 672 1663 \n",
       "Q 744 2109 1322 2381 \n",
       "Q 1416 2425 1619 2425 \n",
       "Q 1816 2425 1919 2381 \n",
       "z\n",
       "M 1619 1850 \n",
       "Q 1572 1850 1516 1819 \n",
       "Q 1341 1719 1266 1553 \n",
       "Q 1234 1488 1234 1397 \n",
       "Q 1234 1263 1309 1097 \n",
       "Q 1406 884 1619 816 \n",
       "Q 1816 881 1928 1097 \n",
       "Q 2006 1253 2006 1397 \n",
       "Q 2006 1472 1972 1553 \n",
       "Q 1900 1716 1722 1819 \n",
       "Q 1663 1850 1619 1850 \n",
       "z\n",
       "M 1757 3605 \n",
       "L 2226 3605 \n",
       "L 2226 3136 \n",
       "L 1757 3136 \n",
       "L 1757 3605 \n",
       "z\n",
       "M 976 3605 \n",
       "L 1445 3605 \n",
       "L 1445 3136 \n",
       "L 976 3136 \n",
       "L 976 3605 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fe98\" d=\"M 934 269 \n",
       "Q 694 0 219 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 47 575 \n",
       "Q 356 575 494 713 \n",
       "Q 647 866 647 1197 \n",
       "L 647 1875 \n",
       "L 1222 1875 \n",
       "L 1222 1197 \n",
       "Q 1222 866 1375 713 \n",
       "Q 1513 575 1822 575 \n",
       "L 1994 575 \n",
       "L 1994 0 \n",
       "L 1650 0 \n",
       "Q 1181 0 934 269 \n",
       "z\n",
       "M 1093 3157 \n",
       "L 1562 3157 \n",
       "L 1562 2688 \n",
       "L 1093 2688 \n",
       "L 1093 3157 \n",
       "z\n",
       "M 312 3157 \n",
       "L 781 3157 \n",
       "L 781 2688 \n",
       "L 312 2688 \n",
       "L 312 3157 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feb4\" d=\"M 3991 -63 \n",
       "Q 3875 -91 3766 -91 \n",
       "Q 3569 -91 3397 -13 \n",
       "Q 2969 178 2913 591 \n",
       "Q 2763 103 2463 0 \n",
       "Q 2250 -75 2047 -75 \n",
       "Q 1741 -75 1516 78 \n",
       "Q 1328 203 1200 431 \n",
       "Q 1066 247 891 144 \n",
       "Q 653 0 344 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 172 575 \n",
       "Q 481 575 619 713 \n",
       "Q 869 963 869 1275 \n",
       "L 869 1875 \n",
       "L 1444 1875 \n",
       "L 1444 1388 \n",
       "Q 1444 1188 1581 875 \n",
       "Q 1716 566 2041 566 \n",
       "Q 2381 566 2519 947 \n",
       "Q 2634 1269 2634 1875 \n",
       "L 3209 1875 \n",
       "Q 3209 1234 3269 1078 \n",
       "Q 3469 547 3813 550 \n",
       "Q 4256 556 4256 1425 \n",
       "L 4256 2344 \n",
       "L 4831 2344 \n",
       "L 4831 1275 \n",
       "Q 4831 972 5091 713 \n",
       "Q 5228 575 5538 575 \n",
       "L 5772 575 \n",
       "L 5772 0 \n",
       "L 5366 0 \n",
       "Q 4913 0 4544 300 \n",
       "Q 4331 19 3991 -63 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fee4\" d=\"M 2713 163 \n",
       "Q 2409 -156 1975 -156 \n",
       "Q 1306 -156 963 150 \n",
       "Q 763 0 438 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 300 575 \n",
       "Q 463 575 600 688 \n",
       "Q 734 797 750 950 \n",
       "Q 813 1516 1234 1766 \n",
       "Q 1528 1941 1819 1941 \n",
       "Q 2044 1941 2250 1850 \n",
       "Q 2853 1591 2906 1069 \n",
       "Q 2922 916 3081 722 \n",
       "Q 3194 584 3528 584 \n",
       "L 3763 584 \n",
       "L 3763 9 \n",
       "L 3356 9 \n",
       "Q 2806 9 2713 163 \n",
       "z\n",
       "M 1269 622 \n",
       "Q 1463 406 1959 406 \n",
       "Q 2106 406 2166 463 \n",
       "Q 2328 628 2328 844 \n",
       "Q 2328 903 2313 959 \n",
       "Q 2244 1234 2063 1288 \n",
       "Q 1941 1325 1819 1325 \n",
       "Q 1653 1325 1525 1241 \n",
       "Q 1369 1138 1322 834 \n",
       "Q 1300 719 1269 622 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fedf\" d=\"M 1347 1159 \n",
       "Q 1347 603 1075 288 \n",
       "Q 825 0 344 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 172 575 \n",
       "Q 481 575 619 713 \n",
       "Q 772 866 772 1197 \n",
       "L 772 4863 \n",
       "L 1347 4863 \n",
       "L 1347 1159 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fee2\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef4\" x=\"66.552734\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fed8\" x=\"96.728516\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe98\" x=\"147.314453\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feb4\" x=\"177.490234\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fee4\" x=\"266.699219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fedf\" x=\"324.511719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"354.980469\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_16\">\n",
       "    <!-- ﺍﺬﻫ -->\n",
       "    <g transform=\"translate(49.458797 60.130087)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-feac\" d=\"M 1706 1797 \n",
       "Q 1528 2178 959 2656 \n",
       "L 1669 2656 \n",
       "Q 1972 2413 2228 1984 \n",
       "Q 2481 1566 2481 1200 \n",
       "Q 2481 972 2741 713 \n",
       "Q 2878 575 3188 575 \n",
       "L 3422 575 \n",
       "L 3422 0 \n",
       "L 3016 0 \n",
       "Q 2556 0 2253 406 \n",
       "Q 1963 6 1316 -97 \n",
       "Q 1172 -119 1031 -119 \n",
       "Q 713 -119 391 0 \n",
       "L 391 575 \n",
       "Q 750 444 1013 444 \n",
       "Q 1116 444 1222 472 \n",
       "Q 1759 625 1869 975 \n",
       "Q 1894 1059 1894 1191 \n",
       "Q 1894 1403 1706 1797 \n",
       "z\n",
       "M 1016 3733 \n",
       "L 1485 3733 \n",
       "L 1485 3264 \n",
       "L 1016 3264 \n",
       "L 1016 3733 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feeb\" d=\"M 1263 841 \n",
       "Q 1375 925 1481 1125 \n",
       "Q 1603 1350 1603 1541 \n",
       "Q 1603 1684 1556 1800 \n",
       "Q 1488 1963 1306 1963 \n",
       "Q 1103 1963 1031 1822 \n",
       "Q 963 1688 963 1578 \n",
       "Q 963 1484 984 1369 \n",
       "Q 1050 1000 1263 841 \n",
       "z\n",
       "M 272 575 \n",
       "Q 434 575 678 609 \n",
       "Q 538 938 491 1153 \n",
       "Q 434 1397 434 1597 \n",
       "Q 434 1859 575 2119 \n",
       "Q 725 2400 966 2372 \n",
       "Q 828 2494 575 2538 \n",
       "L 575 3116 \n",
       "Q 1272 2903 1947 2344 \n",
       "Q 2747 1691 2947 997 \n",
       "Q 2991 834 2991 656 \n",
       "Q 2991 334 2866 144 \n",
       "Q 2634 -209 2088 -209 \n",
       "Q 1678 -209 1197 128 \n",
       "Q 822 0 366 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 272 575 \n",
       "z\n",
       "M 2072 1588 \n",
       "Q 2091 1472 2091 1350 \n",
       "Q 2091 891 1784 563 \n",
       "Q 1747 519 1694 459 \n",
       "Q 1856 316 2009 316 \n",
       "Q 2400 316 2453 525 \n",
       "Q 2475 619 2475 722 \n",
       "Q 2475 909 2372 1153 \n",
       "Q 2281 1369 2072 1588 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feac\" x=\"27.783203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feeb\" x=\"80.273438\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_17\">\n",
       "    <!-- ﻕﺎﻴﺳ -->\n",
       "    <g transform=\"translate(348.524006 80.931509)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fed5\" d=\"M 3437 4053 \n",
       "L 3906 4053 \n",
       "L 3906 3584 \n",
       "L 3437 3584 \n",
       "L 3437 4053 \n",
       "z\n",
       "M 2656 4053 \n",
       "L 3125 4053 \n",
       "L 3125 3584 \n",
       "L 2656 3584 \n",
       "L 2656 4053 \n",
       "z\n",
       "M 3725 1678 \n",
       "Q 3853 1813 3853 2053 \n",
       "Q 3853 2238 3647 2397 \n",
       "Q 3550 2475 3413 2472 \n",
       "Q 3250 2469 3125 2338 \n",
       "Q 3000 2209 2997 2050 \n",
       "Q 2997 1800 3169 1681 \n",
       "Q 3294 1603 3425 1603 \n",
       "Q 3650 1603 3725 1678 \n",
       "z\n",
       "M 1650 -778 \n",
       "Q 2400 -778 2906 -419 \n",
       "Q 3938 313 3938 1338 \n",
       "Q 3741 1103 3359 1103 \n",
       "Q 2944 1103 2709 1313 \n",
       "Q 2403 1584 2403 1988 \n",
       "Q 2403 2150 2428 2288 \n",
       "Q 2500 2750 2969 2966 \n",
       "Q 3213 3078 3450 3078 \n",
       "Q 3759 3078 3938 2922 \n",
       "Q 4259 2641 4381 2269 \n",
       "Q 4484 1950 4484 1450 \n",
       "Q 4484 813 4297 375 \n",
       "Q 4009 -291 3250 -881 \n",
       "Q 2606 -1378 1650 -1375 \n",
       "Q 991 -1372 575 -775 \n",
       "Q 334 -431 334 113 \n",
       "Q 334 675 591 1350 \n",
       "L 1163 1350 \n",
       "Q 897 772 888 288 \n",
       "Q 884 100 922 -63 \n",
       "Q 1088 -778 1650 -778 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feb3\" d=\"M 1200 431 \n",
       "Q 1066 247 891 144 \n",
       "Q 653 0 344 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 172 575 \n",
       "Q 481 575 619 713 \n",
       "Q 869 963 869 1275 \n",
       "L 869 1875 \n",
       "L 1444 1875 \n",
       "L 1444 1388 \n",
       "Q 1444 1188 1581 875 \n",
       "Q 1716 566 2041 566 \n",
       "Q 2381 566 2519 947 \n",
       "Q 2634 1269 2634 1875 \n",
       "L 3209 1875 \n",
       "Q 3209 1234 3269 1078 \n",
       "Q 3469 547 3813 550 \n",
       "Q 4256 556 4256 1425 \n",
       "L 4256 2344 \n",
       "L 4831 2344 \n",
       "L 4831 1275 \n",
       "Q 4831 653 4544 300 \n",
       "Q 4309 13 3991 -63 \n",
       "Q 3875 -91 3766 -91 \n",
       "Q 3569 -91 3397 -13 \n",
       "Q 2969 178 2913 591 \n",
       "Q 2763 103 2463 0 \n",
       "Q 2250 -75 2047 -75 \n",
       "Q 1741 -75 1516 78 \n",
       "Q 1328 203 1200 431 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fed5\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8e\" x=\"77.587891\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef4\" x=\"108.056641\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feb3\" x=\"138.232422\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_18\">\n",
       "    <!-- ﺓﺪﺣﺍﻭ -->\n",
       "    <g transform=\"translate(217.01323 23.183692)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fe93\" d=\"M 1565 3285 \n",
       "L 2034 3285 \n",
       "L 2034 2816 \n",
       "L 1565 2816 \n",
       "L 1565 3285 \n",
       "z\n",
       "M 784 3285 \n",
       "L 1253 3285 \n",
       "L 1253 2816 \n",
       "L 784 2816 \n",
       "L 784 3285 \n",
       "z\n",
       "M 1384 1731 \n",
       "Q 1178 1731 1075 1459 \n",
       "Q 994 1244 997 859 \n",
       "Q 1000 603 1172 466 \n",
       "Q 1350 328 1528 328 \n",
       "Q 1813 328 2103 506 \n",
       "Q 2338 650 2338 916 \n",
       "Q 2338 1156 2131 1359 \n",
       "Q 1747 1734 1384 1731 \n",
       "z\n",
       "M 1272 2294 \n",
       "Q 1891 2294 2459 1794 \n",
       "Q 2900 1406 2900 944 \n",
       "Q 2897 319 2438 84 \n",
       "Q 1922 -181 1509 -181 \n",
       "Q 1209 -181 956 -72 \n",
       "Q 434 159 434 906 \n",
       "Q 434 1472 550 1716 \n",
       "Q 813 2294 1272 2294 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feaa\" d=\"M 1706 1797 \n",
       "Q 1528 2178 959 2656 \n",
       "L 1669 2656 \n",
       "Q 1972 2413 2228 1984 \n",
       "Q 2481 1566 2481 1200 \n",
       "Q 2481 972 2741 713 \n",
       "Q 2878 575 3188 575 \n",
       "L 3422 575 \n",
       "L 3422 0 \n",
       "L 3016 0 \n",
       "Q 2556 0 2253 406 \n",
       "Q 1963 6 1316 -97 \n",
       "Q 1172 -119 1031 -119 \n",
       "Q 713 -119 391 0 \n",
       "L 391 575 \n",
       "Q 750 444 1013 444 \n",
       "Q 1116 444 1222 472 \n",
       "Q 1759 625 1869 975 \n",
       "Q 1894 1059 1894 1191 \n",
       "Q 1894 1403 1706 1797 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fea3\" d=\"M 2659 1691 \n",
       "Q 2400 1763 2078 1834 \n",
       "Q 1813 1894 1250 1944 \n",
       "Q 994 1966 491 1947 \n",
       "L 491 2522 \n",
       "Q 716 2544 950 2547 \n",
       "Q 1456 2547 1994 2438 \n",
       "Q 2800 2275 3488 1969 \n",
       "L 3488 1488 \n",
       "Q 3250 1413 2975 1250 \n",
       "Q 2456 944 2225 738 \n",
       "Q 1772 334 1538 250 \n",
       "Q 847 0 338 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 250 575 \n",
       "Q 959 575 1363 800 \n",
       "Q 1688 981 2050 1313 \n",
       "Q 2344 1581 2659 1691 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feed\" d=\"M 1544 1413 \n",
       "Q 1347 1413 1209 1234 \n",
       "Q 1134 1131 1134 1019 \n",
       "Q 1134 859 1294 713 \n",
       "Q 1441 575 2041 575 \n",
       "Q 2041 1150 1772 1334 \n",
       "Q 1656 1413 1544 1413 \n",
       "z\n",
       "M 2600 575 \n",
       "Q 2600 -388 2063 -888 \n",
       "Q 1766 -1163 1475 -1294 \n",
       "Q 875 -1563 -266 -1563 \n",
       "L -266 -988 \n",
       "Q 856 -988 1275 -750 \n",
       "Q 1800 -453 1994 9 \n",
       "Q 1625 9 1484 41 \n",
       "Q 1016 144 891 250 \n",
       "Q 528 556 528 984 \n",
       "Q 528 1425 806 1697 \n",
       "Q 1131 2019 1559 2019 \n",
       "Q 1838 2019 2063 1859 \n",
       "Q 2469 1578 2547 1131 \n",
       "Q 2600 813 2600 575 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fe93\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feaa\" x=\"52.392578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fea3\" x=\"104.882812\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"166.699219\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feed\" x=\"194.482422\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_19\">\n",
       "    <!-- ﻞﻴﻬﺴﺘﻟﺍ -->\n",
       "    <g transform=\"translate(285.877512 124.044463)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fede\" d=\"M 3256 228 \n",
       "Q 3500 622 3500 1331 \n",
       "L 3500 4863 \n",
       "L 4075 4863 \n",
       "L 4075 1197 \n",
       "Q 4075 866 4228 713 \n",
       "Q 4366 575 4675 575 \n",
       "L 4909 575 \n",
       "L 4909 0 \n",
       "L 4503 0 \n",
       "Q 4153 0 3909 159 \n",
       "Q 3853 16 3769 -109 \n",
       "Q 3391 -663 2575 -869 \n",
       "Q 2156 -975 1891 -975 \n",
       "Q 1581 -975 1366 -906 \n",
       "Q 453 -622 450 234 \n",
       "Q 447 666 647 953 \n",
       "L 1222 953 \n",
       "Q 1019 594 1019 234 \n",
       "Q 1019 -159 1538 -366 \n",
       "Q 1663 -416 1891 -416 \n",
       "Q 2141 -416 2481 -309 \n",
       "Q 3025 -144 3256 228 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-feec\" d=\"M 1059 575 \n",
       "L 1119 575 \n",
       "Q 1250 575 1550 813 \n",
       "Q 1906 1094 1906 1256 \n",
       "Q 1906 1606 1644 1606 \n",
       "Q 1409 1606 1178 1166 \n",
       "Q 1059 941 1059 575 \n",
       "z\n",
       "M 484 575 \n",
       "Q 513 1406 916 1800 \n",
       "Q 1253 2134 1656 2134 \n",
       "Q 2088 2134 2303 1825 \n",
       "Q 2447 1616 2447 1259 \n",
       "Q 2447 913 1938 575 \n",
       "L 3013 575 \n",
       "L 3013 0 \n",
       "L 1938 0 \n",
       "Q 2447 -338 2447 -684 \n",
       "Q 2447 -1041 2303 -1250 \n",
       "Q 2088 -1559 1656 -1559 \n",
       "Q 1253 -1559 916 -1225 \n",
       "Q 513 -825 484 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 484 575 \n",
       "z\n",
       "M 1059 0 \n",
       "Q 1059 -366 1178 -591 \n",
       "Q 1409 -1031 1644 -1031 \n",
       "Q 1906 -1031 1906 -681 \n",
       "Q 1906 -519 1550 -238 \n",
       "Q 1250 0 1119 0 \n",
       "L 1059 0 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fede\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef4\" x=\"75.732422\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feec\" x=\"105.908203\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feb4\" x=\"152.001953\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe98\" x=\"241.210938\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fedf\" x=\"271.386719\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"301.855469\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_20\">\n",
       "    <!-- ﻒﻴﻔﺨﺘﻟﺍ -->\n",
       "    <g transform=\"translate(44.160369 214.756364)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fed2\" d=\"M 5175 113 \n",
       "Q 4850 -22 4506 -97 \n",
       "Q 3631 -284 2781 -284 \n",
       "Q 1828 -281 1438 -159 \n",
       "Q 409 163 406 981 \n",
       "Q 403 1413 603 1700 \n",
       "L 1178 1700 \n",
       "Q 975 1406 975 981 \n",
       "Q 975 622 1609 381 \n",
       "Q 1863 284 2797 284 \n",
       "Q 3522 284 4413 453 \n",
       "L 4500 466 \n",
       "Q 4394 550 4303 688 \n",
       "Q 4047 1075 4047 1453 \n",
       "Q 4047 1550 4059 1663 \n",
       "Q 4128 2147 4716 2381 \n",
       "Q 4897 2453 5059 2453 \n",
       "Q 5316 2453 5569 2297 \n",
       "Q 5997 2038 6053 1569 \n",
       "Q 6066 1453 6066 1359 \n",
       "Q 6066 909 5816 578 \n",
       "Q 5844 575 5866 575 \n",
       "L 6688 575 \n",
       "L 6688 0 \n",
       "L 5850 0 \n",
       "Q 5522 0 5175 113 \n",
       "z\n",
       "M 5175 697 \n",
       "Q 5500 900 5500 1281 \n",
       "Q 5500 1344 5484 1428 \n",
       "Q 5441 1650 5278 1772 \n",
       "Q 5172 1850 5047 1850 \n",
       "Q 4972 1850 4903 1819 \n",
       "Q 4713 1741 4653 1553 \n",
       "Q 4634 1494 4634 1438 \n",
       "Q 4634 1263 4763 1056 \n",
       "Q 4916 813 5175 697 \n",
       "z\n",
       "M 4768 3413 \n",
       "L 5237 3413 \n",
       "L 5237 2944 \n",
       "L 4768 2944 \n",
       "L 4768 3413 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fed4\" d=\"M 1919 2381 \n",
       "Q 2491 2131 2566 1663 \n",
       "Q 2591 1506 2591 1375 \n",
       "Q 2591 1147 2525 994 \n",
       "Q 2434 784 2322 628 \n",
       "Q 2444 575 2903 575 \n",
       "L 3300 575 \n",
       "L 3300 0 \n",
       "L 2622 0 \n",
       "Q 2197 0 1619 178 \n",
       "Q 1041 0 616 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 334 575 \n",
       "Q 794 575 916 628 \n",
       "Q 825 753 713 994 \n",
       "Q 647 1138 647 1375 \n",
       "Q 647 1513 672 1663 \n",
       "Q 744 2109 1322 2381 \n",
       "Q 1416 2425 1619 2425 \n",
       "Q 1816 2425 1919 2381 \n",
       "z\n",
       "M 1619 1850 \n",
       "Q 1572 1850 1516 1819 \n",
       "Q 1341 1719 1266 1553 \n",
       "Q 1234 1488 1234 1397 \n",
       "Q 1234 1263 1309 1097 \n",
       "Q 1406 884 1619 816 \n",
       "Q 1816 881 1928 1097 \n",
       "Q 2006 1253 2006 1397 \n",
       "Q 2006 1472 1972 1553 \n",
       "Q 1900 1716 1722 1819 \n",
       "Q 1663 1850 1619 1850 \n",
       "z\n",
       "M 1384 3605 \n",
       "L 1853 3605 \n",
       "L 1853 3136 \n",
       "L 1384 3136 \n",
       "L 1384 3605 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fea8\" d=\"M 1994 2438 \n",
       "Q 2800 2275 3488 1969 \n",
       "L 3488 1488 \n",
       "Q 3284 1425 3056 1297 \n",
       "Q 3213 1053 3306 947 \n",
       "Q 3631 575 3988 575 \n",
       "L 4194 575 \n",
       "L 4194 0 \n",
       "L 3925 0 \n",
       "Q 3272 0 2878 525 \n",
       "Q 2753 694 2569 997 \n",
       "Q 2353 853 2225 738 \n",
       "Q 1772 334 1538 250 \n",
       "Q 847 0 338 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 250 575 \n",
       "Q 959 575 1363 800 \n",
       "Q 1688 981 2050 1313 \n",
       "Q 2344 1581 2659 1691 \n",
       "Q 2400 1763 2078 1834 \n",
       "Q 1813 1894 1250 1944 \n",
       "Q 994 1966 491 1947 \n",
       "L 491 2522 \n",
       "Q 716 2544 950 2547 \n",
       "Q 1456 2547 1994 2438 \n",
       "z\n",
       "M 1640 3413 \n",
       "L 2109 3413 \n",
       "L 2109 2944 \n",
       "L 1640 2944 \n",
       "L 1640 3413 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fed2\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fef4\" x=\"103.515625\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fed4\" x=\"133.691406\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fea8\" x=\"184.277344\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe98\" x=\"248.828125\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fedf\" x=\"279.003906\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fe8d\" x=\"309.472656\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"text_21\">\n",
       "    <!-- ﺓﺩﻭﺪﻌﻣ -->\n",
       "    <g transform=\"translate(235.396514 17.083636)scale(0.1 -0.1)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-fea9\" d=\"M 1222 472 \n",
       "Q 1759 625 1869 975 \n",
       "Q 1894 1059 1894 1191 \n",
       "Q 1894 1434 1706 1797 \n",
       "Q 1500 2200 959 2656 \n",
       "L 1669 2656 \n",
       "Q 2022 2372 2228 1984 \n",
       "Q 2484 1509 2484 1181 \n",
       "Q 2484 872 2384 638 \n",
       "Q 2134 31 1316 -97 \n",
       "Q 1172 -119 1031 -119 \n",
       "Q 713 -119 391 0 \n",
       "L 391 575 \n",
       "Q 750 444 1013 444 \n",
       "Q 1116 444 1222 472 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fecc\" d=\"M 1544 578 \n",
       "Q 938 0 306 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 238 575 \n",
       "Q 572 575 778 700 \n",
       "Q 963 813 1088 934 \n",
       "Q 784 1200 572 1481 \n",
       "Q 478 1603 478 1744 \n",
       "Q 478 1841 519 1969 \n",
       "Q 569 2141 894 2303 \n",
       "Q 1172 2444 1544 2444 \n",
       "Q 1916 2444 2194 2303 \n",
       "Q 2519 2141 2569 1969 \n",
       "Q 2609 1841 2609 1744 \n",
       "Q 2609 1603 2516 1481 \n",
       "Q 2275 1175 2000 934 \n",
       "Q 2097 838 2309 700 \n",
       "Q 2500 575 2850 575 \n",
       "L 3150 575 \n",
       "L 3150 0 \n",
       "L 2781 0 \n",
       "Q 2150 0 1544 578 \n",
       "z\n",
       "M 1719 1844 \n",
       "Q 1625 1866 1544 1866 \n",
       "Q 1463 1866 1369 1844 \n",
       "Q 1222 1806 1222 1706 \n",
       "Q 1222 1616 1544 1319 \n",
       "Q 1866 1616 1866 1706 \n",
       "Q 1866 1806 1719 1844 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-fee3\" d=\"M 2713 163 \n",
       "Q 2409 -156 1950 -156 \n",
       "Q 1306 -156 963 150 \n",
       "Q 763 0 438 0 \n",
       "L -63 0 \n",
       "L -63 575 \n",
       "L 300 575 \n",
       "Q 463 575 598 686 \n",
       "Q 734 797 750 950 \n",
       "Q 813 1516 1234 1766 \n",
       "Q 1528 1941 1781 1941 \n",
       "Q 2331 1941 2616 1678 \n",
       "Q 2919 1400 2919 825 \n",
       "Q 2919 378 2713 163 \n",
       "z\n",
       "M 1269 622 \n",
       "Q 1463 406 1959 406 \n",
       "Q 2106 406 2166 463 \n",
       "Q 2328 628 2328 844 \n",
       "Q 2328 1078 2203 1197 \n",
       "Q 2072 1325 1797 1325 \n",
       "Q 1653 1325 1511 1231 \n",
       "Q 1369 1138 1322 834 \n",
       "Q 1300 719 1269 622 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-fe93\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fea9\" x=\"52.392578\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feed\" x=\"96.923828\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-feaa\" x=\"145.214844\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fecc\" x=\"197.705078\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-fee3\" x=\"245.947266\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p0ef57a2678\">\n",
       "   <rect x=\"28.942188\" y=\"7.2\" width=\"334.8\" height=\"217.44\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result= compute_pca(X, 2)\n",
    "plt.scatter(result[:, 1], result[:, 0])\n",
    "for i, word in enumerate(words):\n",
    "    reshaped_text = arabic_reshaper.reshape(word)\n",
    "    artext = get_display(reshaped_text)\n",
    "    plt.annotate(artext, xy=(result[i, 1], result[i, 0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaeb31",
   "metadata": {},
   "source": [
    "# Evaluation of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7e325e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between v and w is:  3.4912698182396156\n",
      "The Cosine similarity of v and w is:  0.8611607861266035\n"
     ]
    }
   ],
   "source": [
    "# get vectors\n",
    "v = word_embeddings[st.stem('صراط')]\n",
    "w = word_embeddings[st.stem('مستقيم')]\n",
    "\n",
    "from numpy import linalg\n",
    "\n",
    "# Calculate Euclidean distance d\n",
    "d = linalg.norm(v-w)\n",
    "print(\"The Euclidean distance between v and w is: \", d)\n",
    "\n",
    "# Calculate Cosine similarity c\n",
    "c = np.dot(v,w) / (linalg.norm(v)*linalg.norm(w))\n",
    "print(\"The Cosine similarity of v and w is: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2e4f0eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Euclidean distance between كتب and سياق is: 3.558455696017064\n",
      "The Cosine similarity of كتب and سياق is: 0.8640351648414126\n",
      "\n",
      "The Euclidean distance between سياق and المستقيم is: 3.710402682497853\n",
      "The Cosine similarity of سياق and المستقيم is: 0.8524622705786361\n",
      "\n",
      "The Euclidean distance between المستقيم and التخفيف is: 3.3779984065019826\n",
      "The Cosine similarity of المستقيم and التخفيف is: 0.867442361455463\n",
      "\n",
      "The Euclidean distance between التخفيف and العسر is: 3.6708305689580185\n",
      "The Cosine similarity of التخفيف and العسر is: 0.8406901686117716\n",
      "\n",
      "The Euclidean distance between العسر and الكلام is: 3.5846196137244513\n",
      "The Cosine similarity of العسر and الكلام is: 0.8521924811011359\n",
      "\n",
      "The Euclidean distance between الكلام and واحدة is: 3.4463570878503673\n",
      "The Cosine similarity of الكلام and واحدة is: 0.8628342203548562\n",
      "\n",
      "The Euclidean distance between واحدة and التسهيل is: 3.25142838716164\n",
      "The Cosine similarity of واحدة and التسهيل is: 0.8713164277010864\n",
      "\n",
      "The Euclidean distance between التسهيل and صراط is: 3.749065762868216\n",
      "The Cosine similarity of التسهيل and صراط is: 0.8386509921625515\n",
      "\n",
      "The Euclidean distance between صراط and معدودة is: 3.6817597288016914\n",
      "The Cosine similarity of صراط and معدودة is: 0.8466365392067455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate some word vectors\n",
    "words = ['كُتِب', 'سياق','المستقيم','التخفيف', 'الْعُسرَ','الكلام','واحدة',\n",
    "         'التسهيل','صراط','معدودة']\n",
    "for i, word in enumerate(words):\n",
    "    words[i] = re.sub('[َُِّءًٌٍْ]', '', word)\n",
    "    \n",
    "for i in range(len(words)-1):\n",
    "    v = word_embeddings[words[i]]\n",
    "    w = word_embeddings[words[i+1]]\n",
    "    \n",
    "    # Calculate Euclidean distance d\n",
    "    d = linalg.norm(v-w)\n",
    "    print(f\"The Euclidean distance between {words[i]} and {words[i+1]} is: {d}\")\n",
    "\n",
    "    # Calculate Cosine similarity c\n",
    "    c = np.dot(v,w) / (linalg.norm(v)*linalg.norm(w))\n",
    "    print(f\"The Cosine similarity of {words[i]} and {words[i+1]} is: {c}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "598b1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe out of the dictionary embedding.\n",
    "keys = word_embeddings.keys()\n",
    "df = []\n",
    "for key in keys:\n",
    "    df.append(word_embeddings[key])\n",
    "    \n",
    "embedding = pd.DataFrame(data=df, index=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2cbd0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.256152</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.262998</td>\n",
       "      <td>0.769146</td>\n",
       "      <td>0.575151</td>\n",
       "      <td>0.536670</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.207388</td>\n",
       "      <td>0.575697</td>\n",
       "      <td>0.744846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287068</td>\n",
       "      <td>0.298029</td>\n",
       "      <td>0.608141</td>\n",
       "      <td>0.729462</td>\n",
       "      <td>0.547149</td>\n",
       "      <td>0.203740</td>\n",
       "      <td>0.316043</td>\n",
       "      <td>0.745769</td>\n",
       "      <td>0.575267</td>\n",
       "      <td>0.274717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>..</th>\n",
       "      <td>0.474089</td>\n",
       "      <td>0.346831</td>\n",
       "      <td>0.513005</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>0.750999</td>\n",
       "      <td>0.587764</td>\n",
       "      <td>0.923888</td>\n",
       "      <td>0.778692</td>\n",
       "      <td>0.848825</td>\n",
       "      <td>0.763675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606575</td>\n",
       "      <td>0.766731</td>\n",
       "      <td>0.628832</td>\n",
       "      <td>0.684303</td>\n",
       "      <td>0.389864</td>\n",
       "      <td>0.367853</td>\n",
       "      <td>0.374912</td>\n",
       "      <td>0.287971</td>\n",
       "      <td>0.241111</td>\n",
       "      <td>0.636665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آباؤهن</th>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.736095</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.664454</td>\n",
       "      <td>0.808450</td>\n",
       "      <td>0.532249</td>\n",
       "      <td>0.289360</td>\n",
       "      <td>0.141241</td>\n",
       "      <td>0.541038</td>\n",
       "      <td>0.265466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515674</td>\n",
       "      <td>0.415341</td>\n",
       "      <td>0.469296</td>\n",
       "      <td>0.177325</td>\n",
       "      <td>0.672946</td>\n",
       "      <td>0.673528</td>\n",
       "      <td>0.329231</td>\n",
       "      <td>0.143210</td>\n",
       "      <td>0.421628</td>\n",
       "      <td>0.274753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آبائكم</th>\n",
       "      <td>0.256323</td>\n",
       "      <td>0.831660</td>\n",
       "      <td>0.451752</td>\n",
       "      <td>0.394158</td>\n",
       "      <td>0.710520</td>\n",
       "      <td>0.494641</td>\n",
       "      <td>0.843486</td>\n",
       "      <td>0.647085</td>\n",
       "      <td>0.156937</td>\n",
       "      <td>0.813070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513222</td>\n",
       "      <td>0.484164</td>\n",
       "      <td>0.252853</td>\n",
       "      <td>0.562189</td>\n",
       "      <td>0.285567</td>\n",
       "      <td>0.413538</td>\n",
       "      <td>0.517394</td>\n",
       "      <td>0.767319</td>\n",
       "      <td>0.459870</td>\n",
       "      <td>0.389687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آبائه</th>\n",
       "      <td>0.422039</td>\n",
       "      <td>0.185956</td>\n",
       "      <td>0.413670</td>\n",
       "      <td>0.501493</td>\n",
       "      <td>0.689166</td>\n",
       "      <td>0.467530</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.248635</td>\n",
       "      <td>0.414872</td>\n",
       "      <td>0.095999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289146</td>\n",
       "      <td>0.332645</td>\n",
       "      <td>0.130884</td>\n",
       "      <td>0.411165</td>\n",
       "      <td>0.767012</td>\n",
       "      <td>0.253274</td>\n",
       "      <td>0.516087</td>\n",
       "      <td>0.757594</td>\n",
       "      <td>0.476268</td>\n",
       "      <td>0.454022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آبائهم</th>\n",
       "      <td>0.199850</td>\n",
       "      <td>0.775554</td>\n",
       "      <td>0.553713</td>\n",
       "      <td>0.747809</td>\n",
       "      <td>0.395673</td>\n",
       "      <td>0.180786</td>\n",
       "      <td>0.160946</td>\n",
       "      <td>0.727368</td>\n",
       "      <td>0.504805</td>\n",
       "      <td>0.146942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449376</td>\n",
       "      <td>0.055593</td>\n",
       "      <td>0.459824</td>\n",
       "      <td>0.235720</td>\n",
       "      <td>0.779463</td>\n",
       "      <td>0.712453</td>\n",
       "      <td>0.452862</td>\n",
       "      <td>0.504350</td>\n",
       "      <td>0.383982</td>\n",
       "      <td>0.889707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آباكم</th>\n",
       "      <td>0.365823</td>\n",
       "      <td>0.933367</td>\n",
       "      <td>0.506829</td>\n",
       "      <td>0.613775</td>\n",
       "      <td>0.815193</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.404473</td>\n",
       "      <td>0.470927</td>\n",
       "      <td>0.202103</td>\n",
       "      <td>0.648778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.497204</td>\n",
       "      <td>0.102432</td>\n",
       "      <td>0.248199</td>\n",
       "      <td>0.474122</td>\n",
       "      <td>0.370766</td>\n",
       "      <td>0.175028</td>\n",
       "      <td>0.493862</td>\n",
       "      <td>0.249706</td>\n",
       "      <td>0.528680</td>\n",
       "      <td>0.396553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آباه</th>\n",
       "      <td>0.456806</td>\n",
       "      <td>0.354811</td>\n",
       "      <td>0.272195</td>\n",
       "      <td>0.502853</td>\n",
       "      <td>0.510043</td>\n",
       "      <td>0.521157</td>\n",
       "      <td>0.331822</td>\n",
       "      <td>0.239332</td>\n",
       "      <td>0.951119</td>\n",
       "      <td>0.765883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809065</td>\n",
       "      <td>0.509421</td>\n",
       "      <td>0.367450</td>\n",
       "      <td>0.243268</td>\n",
       "      <td>0.868459</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>0.799260</td>\n",
       "      <td>0.197552</td>\n",
       "      <td>0.074883</td>\n",
       "      <td>0.959256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آباهم</th>\n",
       "      <td>0.298351</td>\n",
       "      <td>0.451349</td>\n",
       "      <td>0.431717</td>\n",
       "      <td>0.396163</td>\n",
       "      <td>0.665032</td>\n",
       "      <td>0.481242</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>0.287674</td>\n",
       "      <td>0.518210</td>\n",
       "      <td>0.141454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637589</td>\n",
       "      <td>0.291312</td>\n",
       "      <td>0.110229</td>\n",
       "      <td>0.684804</td>\n",
       "      <td>0.420546</td>\n",
       "      <td>0.320643</td>\n",
       "      <td>0.551686</td>\n",
       "      <td>0.945757</td>\n",
       "      <td>0.473454</td>\n",
       "      <td>0.557469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>آبية</th>\n",
       "      <td>0.628270</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.530663</td>\n",
       "      <td>0.410842</td>\n",
       "      <td>0.483492</td>\n",
       "      <td>0.209774</td>\n",
       "      <td>0.834970</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.327754</td>\n",
       "      <td>0.499427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480194</td>\n",
       "      <td>0.440001</td>\n",
       "      <td>0.626281</td>\n",
       "      <td>0.744639</td>\n",
       "      <td>0.253115</td>\n",
       "      <td>0.752201</td>\n",
       "      <td>0.798904</td>\n",
       "      <td>0.452005</td>\n",
       "      <td>0.371207</td>\n",
       "      <td>0.958580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6    \\\n",
       ".       0.256152  0.234185  0.262998  0.769146  0.575151  0.536670  0.143765   \n",
       "..      0.474089  0.346831  0.513005  0.559900  0.750999  0.587764  0.923888   \n",
       "آباؤهن  0.436706  0.736095  0.778113  0.664454  0.808450  0.532249  0.289360   \n",
       "آبائكم  0.256323  0.831660  0.451752  0.394158  0.710520  0.494641  0.843486   \n",
       "آبائه   0.422039  0.185956  0.413670  0.501493  0.689166  0.467530  0.333331   \n",
       "آبائهم  0.199850  0.775554  0.553713  0.747809  0.395673  0.180786  0.160946   \n",
       "آباكم   0.365823  0.933367  0.506829  0.613775  0.815193  0.602273  0.404473   \n",
       "آباه    0.456806  0.354811  0.272195  0.502853  0.510043  0.521157  0.331822   \n",
       "آباهم   0.298351  0.451349  0.431717  0.396163  0.665032  0.481242  0.620882   \n",
       "آبية    0.628270  0.578000  0.530663  0.410842  0.483492  0.209774  0.834970   \n",
       "\n",
       "             7         8         9    ...       140       141       142  \\\n",
       ".       0.207388  0.575697  0.744846  ...  0.287068  0.298029  0.608141   \n",
       "..      0.778692  0.848825  0.763675  ...  0.606575  0.766731  0.628832   \n",
       "آباؤهن  0.141241  0.541038  0.265466  ...  0.515674  0.415341  0.469296   \n",
       "آبائكم  0.647085  0.156937  0.813070  ...  0.513222  0.484164  0.252853   \n",
       "آبائه   0.248635  0.414872  0.095999  ...  0.289146  0.332645  0.130884   \n",
       "آبائهم  0.727368  0.504805  0.146942  ...  0.449376  0.055593  0.459824   \n",
       "آباكم   0.470927  0.202103  0.648778  ...  0.497204  0.102432  0.248199   \n",
       "آباه    0.239332  0.951119  0.765883  ...  0.809065  0.509421  0.367450   \n",
       "آباهم   0.287674  0.518210  0.141454  ...  0.637589  0.291312  0.110229   \n",
       "آبية    0.484220  0.327754  0.499427  ...  0.480194  0.440001  0.626281   \n",
       "\n",
       "             143       144       145       146       147       148       149  \n",
       ".       0.729462  0.547149  0.203740  0.316043  0.745769  0.575267  0.274717  \n",
       "..      0.684303  0.389864  0.367853  0.374912  0.287971  0.241111  0.636665  \n",
       "آباؤهن  0.177325  0.672946  0.673528  0.329231  0.143210  0.421628  0.274753  \n",
       "آبائكم  0.562189  0.285567  0.413538  0.517394  0.767319  0.459870  0.389687  \n",
       "آبائه   0.411165  0.767012  0.253274  0.516087  0.757594  0.476268  0.454022  \n",
       "آبائهم  0.235720  0.779463  0.712453  0.452862  0.504350  0.383982  0.889707  \n",
       "آباكم   0.474122  0.370766  0.175028  0.493862  0.249706  0.528680  0.396553  \n",
       "آباه    0.243268  0.868459  0.793958  0.799260  0.197552  0.074883  0.959256  \n",
       "آباهم   0.684804  0.420546  0.320643  0.551686  0.945757  0.473454  0.557469  \n",
       "آبية    0.744639  0.253115  0.752201  0.798904  0.452005  0.371207  0.958580  \n",
       "\n",
       "[10 rows x 150 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a7da6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find K closest words to a vector:\n",
    "def find_closest_word(word, k, embeddings=word_embeddings):\n",
    "    \n",
    "    most_closest_words = []\n",
    "    word_emb = embeddings[word]\n",
    "    similar_word = ''\n",
    "    \n",
    "    for w in embeddings.keys():\n",
    "        if word != w:\n",
    "            # get the word embedding\n",
    "            w_emb = embeddings[w]\n",
    "            # calculating cosine similarity\n",
    "            cur_similarity = cosine_similarity(word_emb, w_emb)\n",
    "            # store the similar_word as a tuple, which contains the word and the similarity\n",
    "            similar_word = (w, cur_similarity)\n",
    "            # append each tuple to list\n",
    "            most_closest_words.append(similar_word)\n",
    "    # sort based on more similarity\n",
    "    most_closest_words.sort(key=lambda y: -y[1])\n",
    "    return most_closest_words[:k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0dbda8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('أخذها', 0.9129466934906125),\n",
       " ('دخلت', 0.9122681003845605),\n",
       " ('حظ', 0.9120120905400518),\n",
       " ('تعلل', 0.9106417968383892),\n",
       " ('يكنى', 0.9096955789853076),\n",
       " ('لدودا', 0.9095126602051142),\n",
       " ('فنحرها', 0.9094109808351455),\n",
       " ('نوعيهما', 0.9084048791513415),\n",
       " ('توطين', 0.9076645041125858),\n",
       " ('يشتمل', 0.9072261384088235),\n",
       " ('بحجه', 0.9070534915280136),\n",
       " ('خطإ', 0.9068875366669401),\n",
       " ('يصف', 0.906665619178433),\n",
       " ('لأملأن', 0.9065464322414832),\n",
       " ('فمخاطب', 0.9060140603715547),\n",
       " ('أهللت', 0.9055880200277683),\n",
       " ('البشرية', 0.9055833322700126),\n",
       " ('مقتدر', 0.9054544869467438),\n",
       " ('ديننا', 0.9051923424947835),\n",
       " ('اتقوني', 0.9047966017047752)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_word('موهبة',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('موهبة',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "693680ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('نازل', 0.9005769064160624),\n",
       " ('الكبير', 0.8980941877702578),\n",
       " ('لئن', 0.8957831571500747),\n",
       " ('قوله', 0.893706924160934),\n",
       " ('أجمل', 0.8929070002422238),\n",
       " ('الاستتار', 0.8919577192935044),\n",
       " ('خطإ', 0.8918843890300903),\n",
       " ('يقتله', 0.8916020577106345),\n",
       " ('بالذوق', 0.8916001896229667),\n",
       " ('للقصاص', 0.8915689690721575),\n",
       " ('استقصا', 0.8913619850241247),\n",
       " ('مرة', 0.8912172187198889),\n",
       " ('الرابعة', 0.8909492933238644),\n",
       " ('ألهمها', 0.8908121564841871),\n",
       " ('نفارق', 0.8908057777896382),\n",
       " ('سيحبط', 0.8906664011440801),\n",
       " ('ليخرج', 0.8904644032034429),\n",
       " ('تخفيفا', 0.8899437587913325),\n",
       " ('المألوفة', 0.8895532412995428),\n",
       " ('مادة', 0.889382447462529)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_word('صراط',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('صراط',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9eb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to store word embeddings result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd5e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'embeddings': word_embeddings}\n",
    "with open('embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('embeddings.pkl', 'rb') as file:\n",
    "#     saved_data = pickle.load(file)\n",
    "\n",
    "# new_embeddings = saved_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce1dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
