{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9640057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install a library to work with docx files\n",
    "# pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e18481f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohammad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from utils import getText, softmax, relu, get_batches, compute_pca, get_dict, cosine_similarity, euclidean_distance\n",
    "import re #  Load the Regex-modul\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3efb58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bf13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get arabic stopwords\n",
    "arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23310448",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/almizan-2.docx'\n",
    "data = getText(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2142ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and process the data\n",
    "def tokenize(corpus):\n",
    "    #  Punktuations are replaced by ' '\n",
    "    data = re.sub(r'[=*\":(),!؟;-]', ' ', corpus)\n",
    "    #  Tokenize string to words\n",
    "    data = ' '.join(data.split('،'))\n",
    "    data = nltk.word_tokenize(data) \n",
    "    for i, word in enumerate(data):\n",
    "        data[i] = re.sub('[ َ ُ ِ ّةء ً ٌ ٍ ْ]', '', word)\n",
    "\n",
    "    #  drop non-alphabetical tokens\n",
    "    data = [ ch for ch in data if not(ch.isnumeric()) or ch.isalpha()]  \n",
    "    data = [ ch for ch in data if ch not in arb_stopwords]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4147452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 94982 \n",
      " ['بسم', 'الله', 'الرحمن', 'الرحيم', 'پايگاه', 'قرآن', 'شناسي', 'حوزه', 'علميه', 'ميبد', 'تفسير', 'الميزان', 'السيد', 'الطباطبائي', 'الجز', 'الثاني', 'سور', 'البقر', 'يأيها', 'امنوا', 'كتب', 'عليكم', 'الصيام', 'كتب', 'قبلكم', 'لعلكم', 'تتقون', 'أياما', 'معدودت', 'منكم']\n"
     ]
    }
   ],
   "source": [
    "data = tokenize(data)\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:30]) #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6c23d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  19308\n",
      "Most frequent tokens:  [('الله', 2444), ('.', 2101), ('تعالى', 1806), ('قوله', 1292), ('قال', 1060), ('الآي', 652), ('السلام', 458), ('الإنسان', 338), ('الناس', 314), ('سبحانه', 280), ('أنه', 267), ('صلى', 257), ('وآله', 249), ('وسلم', 249), ('بن', 239), ('أمر', 230), ('معنى', 224), ('إليه', 218), ('الكلام', 216), ('القرآن', 215)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \", len(fdist))\n",
    "print(\"Most frequent tokens: \", fdist.most_common(20)) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0922823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  21364\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43c975f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the word 'way' :  12578\n",
      "Word which has index 12578:  صراط\n"
     ]
    }
   ],
   "source": [
    "# example of word to index mapping\n",
    "# replace هذا with 'this' because of nice format otherwise we have an ugly format\n",
    "print(f\"Index of the word 'way' :  {word2Ind['صراط']}\")\n",
    "print(f\"Word which has index 12578:  {Ind2word[12578]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f7c085",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b00fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff23516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_W1.shape: (4, 10)\n",
      "tmp_W2.shape: (10, 4)\n",
      "tmp_b1.shape: (4, 1)\n",
      "tmp_b2.shape: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test initialize_model function.\n",
    "tmp_N = 4\n",
    "tmp_V = 10\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
    "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
    "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd8f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "  \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h = relu(h)\n",
    "\n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1fed16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has shape (3, 1)\n",
      "N is 2 and vocabulary size V is 3\n",
      "call forward_prop\n",
      "\n",
      "z has shape (3, 1)\n",
      "z has values:\n",
      "[[0.55379268]\n",
      " [1.58960774]\n",
      " [1.50722933]]\n",
      "\n",
      "h has shape (2, 1)\n",
      "h has values:\n",
      "[[0.92477674]\n",
      " [1.02487333]]\n"
     ]
    }
   ],
   "source": [
    "# Test the forward_prop function\n",
    "\n",
    "# Create some inputs\n",
    "tmp_N = 2\n",
    "tmp_V = 3\n",
    "tmp_x = np.array([[0,1,0]]).T\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
    "\n",
    "print(f\"x has shape {tmp_x.shape}\")\n",
    "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
    "\n",
    "# call function\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(\"call forward_prop\")\n",
    "print()\n",
    "# Look at output\n",
    "print(f\"z has shape {tmp_z.shape}\")\n",
    "print(\"z has values:\")\n",
    "print(tmp_z)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"h has shape {tmp_h.shape}\")\n",
    "print(\"h has values:\")\n",
    "print(tmp_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f3aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = -1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f925d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_x.shape (21364, 4)\n",
      "tmp_y.shape (21364, 4)\n",
      "tmp_W1.shape (50, 21364)\n",
      "tmp_W2.shape (21364, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (21364, 1)\n",
      "tmp_z.shape: (21364, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "tmp_yhat.shape: (21364, 4)\n",
      "call compute_cost\n",
      "tmp_cost 13.0368\n"
     ]
    }
   ],
   "source": [
    "# Test the compute_cost function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "        \n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
    "print(\"call compute_cost\")\n",
    "print(f\"tmp_cost {tmp_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1bebf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # Apply relu to l1\n",
    "    l1 = relu(l1)\n",
    "\n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = (1/batch_size) * np.dot(l1, x.T)\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = (1/batch_size) * np.dot(yhat - y, h.T)\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = (1/batch_size) * np.sum(l1, axis=1, keepdims=True)\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = (1/batch_size) * np.sum((yhat - y), axis=1, keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7299b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get a batch of data\n",
      "tmp_x.shape (21364, 4)\n",
      "tmp_y.shape (21364, 4)\n",
      "\n",
      "Initialize weights and biases\n",
      "tmp_W1.shape (50, 21364)\n",
      "tmp_W2.shape (21364, 50)\n",
      "tmp_b1.shape (50, 1)\n",
      "tmp_b2.shape (21364, 1)\n",
      "\n",
      "Forwad prop to get z and h\n",
      "tmp_z.shape: (21364, 4)\n",
      "tmp_h.shape: (50, 4)\n",
      "\n",
      "Get yhat by calling softmax\n",
      "tmp_yhat.shape: (21364, 4)\n",
      "\n",
      "call back_prop\n",
      "tmp_grad_W1.shape (50, 21364)\n",
      "tmp_grad_W2.shape (21364, 50)\n",
      "tmp_grad_b1.shape (50, 1)\n",
      "tmp_grad_b2.shape (21364, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test the back_prop function\n",
    "tmp_C = 2\n",
    "tmp_N = 50\n",
    "tmp_batch_size = 4\n",
    "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
    "tmp_V = len(word2Ind)\n",
    "\n",
    "\n",
    "# get a batch of data\n",
    "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
    "\n",
    "print(\"get a batch of data\")\n",
    "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
    "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Initialize weights and biases\")\n",
    "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
    "\n",
    "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
    "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
    "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
    "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Forwad prop to get z and h\")\n",
    "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
    "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
    "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
    "\n",
    "print()\n",
    "print(\"Get yhat by calling softmax\")\n",
    "tmp_yhat = softmax(tmp_z)\n",
    "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
    "\n",
    "tmp_m = (2*tmp_C)\n",
    "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
    "\n",
    "print()\n",
    "print(\"call back_prop\")\n",
    "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
    "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
    "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
    "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9c880bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.01, \n",
    "                     random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: implementation of the function to perform forward propagation\n",
    "        softmax: implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "#     batch_size = 512\n",
    "    batch_size = 256\n",
    "    iters = 0\n",
    "    C = 10\n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "                \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "            \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            if cost < 8.0:\n",
    "                alpha *= 0.4\n",
    "            else:\n",
    "                alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53784407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 24.037803\n",
      "iters: 20 cost: 15.549697\n",
      "iters: 30 cost: 26.401212\n",
      "iters: 40 cost: 21.733962\n",
      "iters: 50 cost: 21.932020\n",
      "iters: 60 cost: 12.955119\n",
      "iters: 70 cost: 9.094137\n",
      "iters: 80 cost: 16.779770\n",
      "iters: 90 cost: 14.748732\n",
      "iters: 100 cost: 13.836443\n",
      "iters: 110 cost: 16.422891\n",
      "iters: 120 cost: 7.378118\n",
      "iters: 130 cost: 5.993194\n",
      "iters: 140 cost: 5.649824\n",
      "iters: 150 cost: 14.666813\n",
      "iters: 160 cost: 8.195413\n",
      "iters: 170 cost: 20.653989\n",
      "iters: 180 cost: 19.179364\n",
      "iters: 190 cost: 10.840543\n",
      "iters: 200 cost: 16.612573\n",
      "iters: 210 cost: 19.501566\n",
      "iters: 220 cost: 10.001060\n",
      "iters: 230 cost: 15.930420\n",
      "iters: 240 cost: 4.261406\n",
      "iters: 250 cost: 11.207951\n",
      "iters: 260 cost: 19.882439\n",
      "iters: 270 cost: 15.615543\n",
      "iters: 280 cost: 9.894656\n",
      "iters: 290 cost: 13.299403\n",
      "iters: 300 cost: 14.486851\n",
      "iters: 310 cost: 8.284487\n",
      "iters: 320 cost: 12.977202\n",
      "iters: 330 cost: 14.620620\n",
      "iters: 340 cost: 7.088863\n",
      "iters: 350 cost: 5.461401\n",
      "iters: 360 cost: 7.402425\n",
      "iters: 370 cost: 9.679696\n",
      "iters: 380 cost: 10.129125\n",
      "iters: 390 cost: 9.326167\n",
      "iters: 400 cost: 3.828888\n",
      "iters: 410 cost: 5.783771\n",
      "iters: 420 cost: 8.160645\n",
      "iters: 430 cost: 15.531154\n",
      "iters: 440 cost: 11.974209\n",
      "iters: 450 cost: 12.283279\n",
      "iters: 460 cost: 14.962401\n",
      "iters: 470 cost: 16.497682\n",
      "iters: 480 cost: 5.222147\n",
      "iters: 490 cost: 17.512526\n",
      "iters: 500 cost: 15.947206\n",
      "iters: 510 cost: 9.415455\n",
      "iters: 520 cost: 3.793593\n",
      "iters: 530 cost: 14.813358\n",
      "iters: 540 cost: 4.134211\n",
      "iters: 550 cost: 5.488062\n",
      "iters: 560 cost: 12.582643\n",
      "iters: 570 cost: 17.455424\n",
      "iters: 580 cost: 9.799815\n",
      "iters: 590 cost: 6.238394\n",
      "iters: 600 cost: 16.349457\n",
      "iters: 610 cost: 9.146859\n",
      "iters: 620 cost: 3.944616\n",
      "iters: 630 cost: 13.449315\n",
      "iters: 640 cost: 14.074741\n",
      "iters: 650 cost: 16.273148\n",
      "iters: 660 cost: 9.877580\n",
      "iters: 670 cost: 13.242852\n",
      "iters: 680 cost: 9.498529\n",
      "iters: 690 cost: 17.085204\n",
      "iters: 700 cost: 19.665771\n",
      "iters: 710 cost: 9.862822\n",
      "iters: 720 cost: 16.645874\n",
      "iters: 730 cost: 9.930041\n",
      "iters: 740 cost: 12.948203\n",
      "iters: 750 cost: 13.810449\n",
      "iters: 760 cost: 16.231592\n",
      "iters: 770 cost: 16.234414\n",
      "iters: 780 cost: 10.506581\n",
      "iters: 790 cost: 9.878903\n",
      "iters: 800 cost: 4.347002\n",
      "iters: 810 cost: 15.463134\n",
      "iters: 820 cost: 4.803240\n",
      "iters: 830 cost: 15.192695\n",
      "iters: 840 cost: 21.237466\n",
      "iters: 850 cost: 14.137194\n",
      "iters: 860 cost: 13.701702\n",
      "iters: 870 cost: 12.372036\n",
      "iters: 880 cost: 15.411393\n",
      "iters: 890 cost: 12.317145\n",
      "iters: 900 cost: 18.228372\n",
      "iters: 910 cost: 13.019700\n",
      "iters: 920 cost: 16.610913\n",
      "iters: 930 cost: 15.970078\n",
      "iters: 940 cost: 17.509047\n",
      "iters: 950 cost: 12.760755\n",
      "iters: 960 cost: 10.219271\n",
      "iters: 970 cost: 13.700225\n",
      "iters: 980 cost: 9.241354\n",
      "iters: 990 cost: 10.943656\n",
      "iters: 1000 cost: 20.112974\n"
     ]
    }
   ],
   "source": [
    "# test gradient_descent function\n",
    "data = getText(path)\n",
    "data = tokenize(data)\n",
    "C = 2\n",
    "N = 300\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 1000\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cc69b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To extract word vectors we have three approaches\n",
    "# First approach: to assume W1 as word vectors \n",
    "# embs = W1.T\n",
    "\n",
    "# Second approach: to assume W2 as word vectors \n",
    "# embs = W2\n",
    "\n",
    "# Third approach: or to assume the mean of W1+W2 as word vectors \n",
    "embs = (W1.T + W2)/2.0\n",
    "\n",
    "word_embeddings = {}\n",
    "for i, vec in enumerate(embs):\n",
    "    word_embeddings[Ind2word[i]] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75434b8",
   "metadata": {},
   "source": [
    "# Visualizing the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaedd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 2 libraries for dispalying arabic text on a plot properly\n",
    "# pip install python-bidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bed139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade arabic-reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee149fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the word vectors here\n",
    "from bidi.algorithm import get_display\n",
    "import matplotlib.pyplot as plt\n",
    "import arabic_reshaper\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "words = ['كُتِب', 'صراط','الآية','المستقيم', 'الْعُسرَ','الكلام','واحدة',\n",
    "         'التسهيل','التخفيف','معدودة']\n",
    " \n",
    "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1be947",
   "metadata": {},
   "outputs": [],
   "source": [
    "result= compute_pca(X, 4)\n",
    "plt.scatter(result[:, 1], result[:, 3])\n",
    "for i, word in enumerate(words):\n",
    "    reshaped_text = arabic_reshaper.reshape(word)\n",
    "    artext = get_display(reshaped_text)\n",
    "    plt.annotate(artext, xy=(result[i, 1], result[i, 3]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbaeb31",
   "metadata": {},
   "source": [
    "# Evaluation of word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e325e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vectors\n",
    "v = word_embeddings['صراط']\n",
    "w = word_embeddings['مستقيم']\n",
    "\n",
    "from numpy import linalg\n",
    "\n",
    "# Calculate Euclidean distance d\n",
    "d = linalg.norm(v-w)\n",
    "print(\"The Euclidean distance between v and w is: \", d)\n",
    "\n",
    "# Calculate Cosine similarity c\n",
    "c = np.dot(v,w) / (linalg.norm(v)*linalg.norm(w))\n",
    "print(\"The Cosine similarity of v and w is: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate some word vectors\n",
    "words = ['كُتِب', 'سياق','المستقيم','صراط', 'الْعُسرَ','الكلام','واحدة',\n",
    "         'التسهيل','التخفيف','معدودة']\n",
    "for i in range(len(words)-1):\n",
    "    v = word_embeddings[words[i]]\n",
    "    w = word_embeddings[words[i+1]]\n",
    "    \n",
    "    # Calculate Euclidean distance d\n",
    "    d = linalg.norm(v-w)\n",
    "    print(f\"The Euclidean distance between {words[i]} and {words[i+1]} is: {d}\")\n",
    "\n",
    "    # Calculate Cosine similarity c\n",
    "    c = np.dot(v,w) / (linalg.norm(v)*linalg.norm(w))\n",
    "    print(f\"The Cosine similarity of {words[i]} and {words[i+1]} is: {c}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe out of the dictionary embedding.\n",
    "keys = word_embeddings.keys()\n",
    "df = []\n",
    "for key in keys:\n",
    "    df.append(word_embeddings[key])\n",
    "    \n",
    "embedding = pd.DataFrame(data=df, index=keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find K closest words to a vector:\n",
    "def find_closest_word(word, k, embeddings=word_embeddings):\n",
    "    \n",
    "    most_closest_words = []\n",
    "    word_emb = embeddings[word]\n",
    "    similar_word = ''\n",
    "    \n",
    "    for w in embeddings.keys():\n",
    "        if word != w:\n",
    "            # get the word embedding\n",
    "            w_emb = embeddings[w]\n",
    "            # calculating cosine similarity\n",
    "            cur_similarity = cosine_similarity(word_emb, w_emb)\n",
    "            # store the similar_word as a tuple, which contains the word and the similarity\n",
    "            similar_word = (w, cur_similarity)\n",
    "            # append each tuple to list\n",
    "            most_closest_words.append(similar_word)\n",
    "    # sort based on more similarity\n",
    "    most_closest_words.sort(key=lambda y: -y[1])\n",
    "    return most_closest_words[:k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbda8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('موهبة',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('موهبة',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693680ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('صراط',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word('صراط',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff0d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee9eb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # to store word embeddings result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9cd5e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'embeddings': word_embeddings}\n",
    "with open('embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ae8cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('embeddings.pkl', 'rb') as file:\n",
    "#     saved_data = pickle.load(file)\n",
    "\n",
    "# new_embeddings = saved_data['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ce1dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
