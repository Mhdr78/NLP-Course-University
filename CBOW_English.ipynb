{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05589135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "# reading text from docx\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab6a1322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getText('./shakespeare.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66a3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mohammad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils import softmax, relu, get_batches, compute_pca, get_dict\n",
    "import re #  Load the Regex-modul\n",
    "\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771a52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sentence tokenizer\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b175a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and process the data\n",
    "def tokenize(corpus):\n",
    "    data = re.sub(r'[,!?;-]', '.',corpus)                                 #  Punktuations are replaced by .\n",
    "    data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
    "    data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e57b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60976 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention', '.', 'a', 'kingdom', 'for', 'a', 'stage', '.', 'princes', 'to', 'act', 'and', 'monarchs', 'to', 'behold', 'the']\n"
     ]
    }
   ],
   "source": [
    "data = tokenize(data)\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:30]) #  print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b9bff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(\"Size of vocabulary: \", len(fdist))\n",
    "print(\"Most frequent tokens: \", fdist.most_common(20)) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2346fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n"
     ]
    }
   ],
   "source": [
    "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "041379b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1dbc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "     Outputs: \n",
    "        z:  output score vector\n",
    "    '''\n",
    "    \n",
    "    # Calculate h\n",
    "    h = np.dot(W1, x) + b1\n",
    "  \n",
    "    # Apply the relu on h, \n",
    "    # store the relu in h\n",
    "    h = relu(h)\n",
    "\n",
    "    # Calculate z\n",
    "    z = np.dot(W2, h) + b2\n",
    "\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73c2f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_cost: cross-entropy cost function\n",
    "def compute_cost(y, yhat, batch_size):\n",
    "\n",
    "    # cost function \n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63c4ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "     Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "    # Compute l1 as W2^T (Yhat - Y)\n",
    "    l1 = np.dot(W2.T, yhat - y)\n",
    "\n",
    "    # Apply relu to l1\n",
    "    l1 = relu(l1)\n",
    "\n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = (1/batch_size) * np.dot(l1, x.T)\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = (1/batch_size) * np.dot(yhat - y, h.T)\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = (1/batch_size) * np.sum(l1, axis=1, keepdims=True)\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = (1/batch_size) * np.sum((yhat - y), axis=1, keepdims=True)\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d22b06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# UNQ_C5 GRADED FUNCTION: gradient_descent\n",
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
    "                     random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    '''\n",
    "    This is the gradient_descent function\n",
    "    \n",
    "      Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        V:         dimension of vocabulary \n",
    "        num_iters: number of iterations  \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        softmax: your implementation of the softmax function\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "     Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
    "\n",
    "    batch_size = 128\n",
    "#    batch_size = 512\n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    \n",
    "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your own code) ###                \n",
    "        # get z and h\n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "                \n",
    "        # get yhat\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "            \n",
    "        # get gradients\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e45d946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 8.538367\n",
      "iters: 20 cost: 4.449100\n",
      "iters: 30 cost: 16.154438\n",
      "iters: 40 cost: 2.372795\n",
      "iters: 50 cost: 10.508012\n",
      "iters: 60 cost: 7.730859\n",
      "iters: 70 cost: 5.893077\n",
      "iters: 80 cost: 9.354901\n",
      "iters: 90 cost: 10.002799\n",
      "iters: 100 cost: 11.484674\n",
      "iters: 110 cost: 4.625150\n",
      "iters: 120 cost: 4.428295\n",
      "iters: 130 cost: 10.306100\n",
      "iters: 140 cost: 6.705970\n",
      "iters: 150 cost: 3.189159\n",
      "iters: 160 cost: 10.728620\n",
      "iters: 170 cost: 9.921070\n",
      "iters: 180 cost: 6.962028\n",
      "iters: 190 cost: 9.381010\n",
      "iters: 200 cost: 8.728530\n",
      "iters: 210 cost: 9.456812\n",
      "iters: 220 cost: 7.209664\n",
      "iters: 230 cost: 8.731098\n",
      "iters: 240 cost: 9.219848\n",
      "iters: 250 cost: 3.614170\n",
      "iters: 260 cost: 8.336940\n",
      "iters: 270 cost: 10.298402\n",
      "iters: 280 cost: 9.063663\n",
      "iters: 290 cost: 5.041881\n",
      "iters: 300 cost: 8.813947\n",
      "iters: 310 cost: 9.597524\n",
      "iters: 320 cost: 8.715906\n",
      "iters: 330 cost: 8.526511\n",
      "iters: 340 cost: 8.767531\n",
      "iters: 350 cost: 8.471202\n",
      "iters: 360 cost: 9.213956\n",
      "iters: 370 cost: 4.532446\n",
      "iters: 380 cost: 9.074484\n",
      "iters: 390 cost: 9.432701\n",
      "iters: 400 cost: 8.497945\n",
      "iters: 410 cost: 6.158831\n",
      "iters: 420 cost: 8.177446\n",
      "iters: 430 cost: 6.817622\n",
      "iters: 440 cost: 6.961463\n",
      "iters: 450 cost: 7.890127\n",
      "iters: 460 cost: 5.687550\n",
      "iters: 470 cost: 8.882412\n",
      "iters: 480 cost: 4.262432\n",
      "iters: 490 cost: 8.735137\n",
      "iters: 500 cost: 6.678940\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "# UNIT TEST COMMENT: Each time this cell is run the cost for each iteration changes slightly (the change is less dramatic after some iterations)\n",
    "# to have this into account let's accept an answer as correct if the cost of iter 15 = 41.6 (without caring about decimal points beyond the first decimal)\n",
    "# 41.66, 41.69778, 41.63, etc should all be valid answers.\n",
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 500\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
